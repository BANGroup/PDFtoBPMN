# Docker Compose для Qwen VLM OCR сервиса
#
# ═══════════════════════════════════════════════════════════════════
# АРХИТЕКТУРА OCR СЕРВИСОВ (финальная версия 27.01.2026)
# ═══════════════════════════════════════════════════════════════════
#
# 1. Qwen2-VL-2B (Docker) - УНИВЕРСАЛЬНЫЙ
#    - Работает без flash_attn через SDPA
#    - Подходит для любых GPU с 8GB+ VRAM
#    - Профили: default (Ada), blackwell (RTX 5070/5080/5090)
#
# 2. Qwen2-VL-7B (ЛОКАЛЬНО) - только для машин с 24GB+ VRAM
#    - Требует RTX 5090 / A6000 / H100
#    - Рекомендуется локальная установка
#
# 3. DeepSeek-OCR (ЛОКАЛЬНО) - только с flash_attn
#    - Требует предкомпилированный flash_attn под конкретную GPU
#    - Не подходит для Docker из-за CUDA OOM без flash_attn
#    - Дает bbox координаты (Qwen не дает)
#
# GPU совместимость Docker образов:
#   - RTX 4080/4090 (Ada sm_89): qwen-vlm-service:2b (PyTorch 2.5+cu124)
#   - RTX 5070/5080/5090 (Blackwell sm_120): qwen-vlm-service:2b-cu128 (PyTorch 2.10+cu128)
#
# Профили:
#   - default: Qwen2-VL-2B для Ada GPUs (RTX 4080/4090)
#   - blackwell: Qwen2-VL-2B для Blackwell GPUs (RTX 5070/5080/5090)
#
# Использование:
#   docker compose up                         # 2B модель (Ada GPUs)
#   docker compose --profile blackwell up     # 2B модель (Blackwell GPUs)
#
# Конфигурация:
#   1. Скопировать ../.env.example в ../.env
#   2. Заполнить HF_TOKEN и другие переменные
#   3. docker compose автоматически прочитает ../.env

version: "3.8"

services:
  # ============================================================
  # Qwen VLM Service (2B - для Ada GPUs: RTX 4080/4090)
  # ============================================================
  qwen-vlm-2b:
    build:
      context: ./qwen-vlm-service
      dockerfile: Dockerfile
    image: qwen-vlm-service:2b
    container_name: qwen-vlm-2b
    env_file:
      - ../.env  # Секреты из корня проекта
    ports:
      - "8001:8000"
    environment:
      - QWEN_MODEL=Qwen/Qwen2-VL-2B-Instruct
      - MAX_NEW_TOKENS=2048
      - USE_FLASH_ATTENTION=false
    volumes:
      - qwen-models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # ============================================================
  # Qwen VLM Service (2B - для Blackwell GPUs: RTX 5070/5080/5090)
  # ============================================================
  qwen-vlm-2b-blackwell:
    image: qwen-vlm-service:2b-cu128
    container_name: qwen-vlm-2b-blackwell
    env_file:
      - ../.env
    profiles:
      - blackwell
    ports:
      - "8001:8000"
    environment:
      - QWEN_MODEL=Qwen/Qwen2-VL-2B-Instruct
      - MAX_NEW_TOKENS=2048
      - USE_FLASH_ATTENTION=false
    volumes:
      - qwen-models:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # ═══════════════════════════════════════════════════════════════
  # DEPRECATED: Qwen 7B и DeepSeek рекомендуется использовать локально
  # ═══════════════════════════════════════════════════════════════
  #
  # Qwen 7B:
  #   - Требует 24GB+ VRAM (RTX 5090, A6000)
  #   - Локальная установка: pip install transformers accelerate qwen-vl-utils
  #   - Запуск: python scripts/pdf_to_context/ocr_service/qwen_local_service.py
  #
  # DeepSeek-OCR:
  #   - Требует flash_attn под конкретную GPU
  #   - Без flash_attn → CUDA OOM даже на 16GB
  #   - Локальная установка в DeepSeek-OCR/venv/
  #   - Запуск: cd DeepSeek-OCR && source venv/bin/activate && python ocr_server_api.py
  #
  # Эти сервисы закомментированы, но сохранены для справки.
  # ═══════════════════════════════════════════════════════════════

volumes:
  qwen-models:
    name: qwen-vlm-models
    driver: local
