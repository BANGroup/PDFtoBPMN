#!/usr/bin/env python3
"""
Qwen VLM OCR Service - FastAPI –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è VLM OCR

–†–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–µ—Ç—Å—è –≤ Docker –Ω–∞ –º–∞—à–∏–Ω–µ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º VRAM (24GB+ –¥–ª—è 7B).
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç REST API –¥–ª—è OCR –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

Endpoints:
    POST /ocr - —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    GET /health - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞
    GET /info - –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
"""

import os
import io
import base64
import time
from typing import Optional
from contextlib import asynccontextmanager

import torch
from PIL import Image
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ environment variables
MODEL_NAME = os.getenv("QWEN_MODEL", "Qwen/Qwen2-VL-7B-Instruct")
USE_FLASH_ATTENTION = os.getenv("USE_FLASH_ATTENTION", "false").lower() == "true"
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "2048"))
MAX_PIXELS = int(os.getenv("MAX_PIXELS", str(1280 * 28 * 28)))
MIN_PIXELS = int(os.getenv("MIN_PIXELS", str(256 * 28 * 28)))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
processor = None
device_info = {}


class OCRRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ OCR"""
    image: str  # Base64 encoded image
    prompt: Optional[str] = None
    language: str = "russian"
    max_tokens: Optional[int] = None


class OCRResponse(BaseModel):
    """–û—Ç–≤–µ—Ç OCR"""
    text: str
    model: str
    inference_time: float
    vram_used_gb: float


class HealthResponse(BaseModel):
    """–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
    status: str
    model: str
    vram_total_gb: float
    vram_free_gb: float
    cuda_available: bool


class InfoResponse(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–∏—Å–µ"""
    model: str
    max_tokens: int
    flash_attention: bool
    cuda_version: str
    torch_version: str


# –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤
PROMPTS = {
    "russian": "–ü—Ä–æ—á–∏—Ç–∞–π –∏ –∏–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Å–ø–∏—Å–∫–∏, —Ç–∞–±–ª–∏—Ü—ã). –§–æ—Ä–º–∞—Ç: Markdown.",
    "english": "Read and extract all text from this document. Preserve structure (headers, lists, tables). Format: Markdown.",
    "auto": "Extract all text from this image. Preserve document structure. Output format: Markdown."
}


def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
    global model, processor, device_info
    
    print(f"üöÄ Loading model: {MODEL_NAME}")
    print(f"   Flash Attention: {USE_FLASH_ATTENTION}")
    print(f"   Max tokens: {MAX_NEW_TOKENS}")
    
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏
    load_kwargs = {
        "torch_dtype": torch.bfloat16,
        "device_map": "auto",
        "low_cpu_mem_usage": True,
    }
    
    if USE_FLASH_ATTENTION:
        load_kwargs["attn_implementation"] = "flash_attention_2"
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_NAME,
        **load_kwargs
    )
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    processor = AutoProcessor.from_pretrained(
        MODEL_NAME,
        min_pixels=MIN_PIXELS,
        max_pixels=MAX_PIXELS
    )
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
    if torch.cuda.is_available():
        device_info = {
            "cuda_available": True,
            "vram_total": torch.cuda.mem_get_info()[1] / 1024**3,
            "vram_free": torch.cuda.mem_get_info()[0] / 1024**3,
            "cuda_version": torch.version.cuda,
        }
    else:
        device_info = {
            "cuda_available": False,
            "vram_total": 0,
            "vram_free": 0,
            "cuda_version": "N/A",
        }
    
    print(f"‚úÖ Model loaded! VRAM: {device_info.get('vram_free', 0):.1f}GB free")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle management"""
    load_model()
    yield
    # Cleanup
    global model, processor
    del model
    del processor
    torch.cuda.empty_cache()


app = FastAPI(
    title="Qwen VLM OCR Service",
    description="VLM-based OCR service using Qwen2-VL models",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/health", response_model=HealthResponse)
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    vram_free = 0
    vram_total = 0
    if torch.cuda.is_available():
        vram_free = torch.cuda.mem_get_info()[0] / 1024**3
        vram_total = torch.cuda.mem_get_info()[1] / 1024**3
    
    return HealthResponse(
        status="healthy",
        model=MODEL_NAME,
        vram_total_gb=round(vram_total, 2),
        vram_free_gb=round(vram_free, 2),
        cuda_available=torch.cuda.is_available()
    )


@app.get("/info", response_model=InfoResponse)
async def info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–∏—Å–µ"""
    return InfoResponse(
        model=MODEL_NAME,
        max_tokens=MAX_NEW_TOKENS,
        flash_attention=USE_FLASH_ATTENTION,
        cuda_version=device_info.get("cuda_version", "N/A"),
        torch_version=torch.__version__
    )


@app.post("/ocr", response_model=OCRResponse)
async def ocr(request: OCRRequest):
    """
    OCR –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    
    Args:
        request: OCRRequest —Å base64-encoded –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        
    Returns:
        OCRResponse —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
    """
    if model is None or processor is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_data = base64.b64decode(request.image)
        image = Image.open(io.BytesIO(image_data)).convert("RGB")
        
        # –í—ã–±–æ—Ä –ø—Ä–æ–º–ø—Ç–∞
        prompt = request.prompt or PROMPTS.get(request.language, PROMPTS["auto"])
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        from qwen_vl_utils import process_vision_info
        
        messages = [{"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": prompt}
        ]}]
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        text = processor.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        image_inputs, _ = process_vision_info(messages)
        inputs = processor(
            text=[text], 
            images=image_inputs, 
            padding=True, 
            return_tensors="pt"
        ).to("cuda" if torch.cuda.is_available() else "cpu")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        start_time = time.time()
        
        max_tokens = request.max_tokens or MAX_NEW_TOKENS
        outputs = model.generate(
            **inputs, 
            max_new_tokens=max_tokens,
            do_sample=False
        )
        
        inference_time = time.time() - start_time
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        result = processor.batch_decode(
            [outputs[0][len(inputs.input_ids[0]):]],
            skip_special_tokens=True
        )[0]
        
        # VRAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
        vram_used = 0
        if torch.cuda.is_available():
            vram_used = (torch.cuda.mem_get_info()[1] - torch.cuda.mem_get_info()[0]) / 1024**3
        
        return OCRResponse(
            text=result,
            model=MODEL_NAME,
            inference_time=round(inference_time, 2),
            vram_used_gb=round(vram_used, 2)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
