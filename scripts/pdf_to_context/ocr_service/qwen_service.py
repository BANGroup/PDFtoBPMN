"""
Qwen2.5-VL OCR Service - –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π VLM-based OCR

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen2.5-VL –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è:
- –¢–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
- –¢–∞–±–ª–∏—Ü –∏ —Ñ–æ—Ä–º—É–ª
- –î–∏–∞–≥—Ä–∞–º–º –∏ —Å—Ö–µ–º

–ü–†–ò–ù–¶–ò–ü –û–ë–†–ê–¢–ù–û–ô –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò:
- –≠—Ç–æ –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê DeepSeek-OCR, –Ω–µ –∑–∞–º–µ–Ω–∞
- –í–∫–ª—é—á–∞–µ—Ç—Å—è —è–≤–Ω–æ —á–µ—Ä–µ–∑ OCRServiceFactory.create(service_type="qwen")
- –ï—Å–ª–∏ transformers/torch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã ‚Üí graceful degradation

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ DeepSeek-OCR:
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Markdown output –∏–∑ –∫–æ—Ä–æ–±–∫–∏
- Multi-language (–≤–∫–ª—é—á–∞—è —Ä—É—Å—Å–∫–∏–π)
- –ê–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è (Qwen team)

–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
- –¢—Ä–µ–±—É–µ—Ç ~16GB VRAM (7B –º–æ–¥–µ–ª—å)
- –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ
"""

import os
import io
import base64
from typing import Optional

from .base import OCRService

# Graceful imports
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    Qwen2VLForConditionalGeneration = None
    AutoProcessor = None

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    Image = None


class QwenVLService(OCRService):
    """
    OCR —Å–µ—Ä–≤–∏—Å –Ω–∞ –±–∞–∑–µ Qwen2.5-VL
    
    –ú–æ–¥–µ–ª–∏:
    - Qwen/Qwen2-VL-2B-Instruct (2B, ~4-5GB VRAM) - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è
    - Qwen/Qwen2-VL-7B-Instruct (7B, ~16GB VRAM)
    - Qwen/Qwen2.5-VL-7B-Instruct (7B, ~16GB VRAM) - —Ç—Ä–µ–±—É–µ—Ç flash_attn
    
    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Qwen2-VL (–Ω–µ 2.5) –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞ —Å transformers 5.0
    """
    
    # –ú–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    DEFAULT_MODEL = "Qwen/Qwen2-VL-2B-Instruct"  # –°—Ç–∞–±–∏–ª—å–Ω–∞—è, —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ flash_attn
    LARGE_MODEL = "Qwen/Qwen2-VL-7B-Instruct"
    
    # –ü—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    PROMPTS = {
        "default": "Please extract all text from this image. Output in Markdown format.",
        "ocr_simple": "Extract all visible text from this image exactly as shown.",
        "parse_figure": "Describe this figure/diagram in detail. Include any text labels, arrows, and relationships.",
        "table": "Extract this table to Markdown format. Preserve the structure accurately.",
        "bpmn": "This is a BPMN diagram. Describe all elements: tasks, gateways, events, flows, and swimlanes.",
        "formula": "Extract the mathematical formula/equation from this image in LaTeX format.",
        "russian": "–ò–∑–≤–ª–µ–∫–∏—Ç–µ –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞: Markdown.",
    }
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        device: str = "auto",
        torch_dtype: str = "auto",
        max_new_tokens: int = 2048,
        use_flash_attention: bool = False  # –í—ã–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (—Ç—Ä–µ–±—É–µ—Ç pip install flash-attn)
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qwen VL —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ HuggingFace (None = default)
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('auto', 'cuda', 'cpu')
            torch_dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö ('auto', 'float16', 'bfloat16')
            max_new_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            use_flash_attention: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Flash Attention 2 (—Ç—Ä–µ–±—É–µ—Ç pip install flash-attn)
        """
        self.model_name = model_name or self.DEFAULT_MODEL
        self.device = device
        self.torch_dtype = torch_dtype
        self.max_new_tokens = max_new_tokens
        self.use_flash_attention = use_flash_attention
        
        self._model = None
        self._processor = None
        self._available = None
        self._actual_device = None
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
        if self._available is not None:
            return self._available
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        if not TORCH_AVAILABLE:
            self._available = False
            return False
        
        if not TRANSFORMERS_AVAILABLE:
            self._available = False
            return False
        
        if not PIL_AVAILABLE:
            self._available = False
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
        if not torch.cuda.is_available():
            print("‚ö†Ô∏è QwenVL: CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")
        
        self._available = True
        return True
    
    def _load_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if self._model is not None:
            return
        
        if not self.is_available():
            raise RuntimeError(
                "QwenVL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!\n"
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch accelerate"
            )
        
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name}...")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ dtype
        if self.device == "auto":
            self._actual_device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self._actual_device = self.device
        
        if self.torch_dtype == "auto":
            dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
        elif self.torch_dtype == "float16":
            dtype = torch.float16
        elif self.torch_dtype == "bfloat16":
            dtype = torch.bfloat16
        else:
            dtype = torch.float32
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
        load_kwargs = {
            "torch_dtype": dtype,
            "device_map": "auto" if self._actual_device == "cuda" else None,
        }
        
        # Flash Attention 2 (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.use_flash_attention and torch.cuda.is_available():
            try:
                load_kwargs["attn_implementation"] = "flash_attention_2"
            except Exception:
                pass
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self._model = Qwen2VLForConditionalGeneration.from_pretrained(
            self.model_name,
            **load_kwargs
        )
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        self._processor = AutoProcessor.from_pretrained(self.model_name)
        
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–µ—Å–ª–∏ –Ω–µ auto device_map)
        if load_kwargs.get("device_map") is None:
            self._model.to(self._actual_device)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self._actual_device}")
    
    def process_image(self, image_data: bytes, prompt: str = "") -> str:
        """
        OCR —á–µ—Ä–µ–∑ Qwen2.5-VL
        
        Args:
            image_data: –ë–∞–π—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            prompt: –¢–∏–ø –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ Markdown
        """
        if not self.is_available():
            raise RuntimeError("QwenVL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        self._load_model()
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            if prompt in self.PROMPTS:
                actual_prompt = self.PROMPTS[prompt]
            elif prompt:
                actual_prompt = prompt
            else:
                actual_prompt = self.PROMPTS["default"]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64
            image_base64 = base64.b64encode(image_data).decode("utf-8")
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": f"data:image/png;base64,{image_base64}",
                        },
                        {
                            "type": "text",
                            "text": actual_prompt
                        }
                    ]
                }
            ]
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            text = self._processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # –û—Ç–∫—Ä—ã—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            pil_image = Image.open(io.BytesIO(image_data))
            
            inputs = self._processor(
                text=[text],
                images=[pil_image],
                padding=True,
                return_tensors="pt"
            )
            
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = inputs.to(self._model.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                output_ids = self._model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens,
                    do_sample=False,
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated_ids = output_ids[:, inputs.input_ids.shape[1]:]
            output_text = self._processor.batch_decode(
                generated_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]
            
            return output_text.strip()
        
        except Exception as e:
            raise RuntimeError(f"QwenVL –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
    
    def get_service_name(self) -> str:
        return f"Qwen2.5-VL ({self.model_name.split('/')[-1]})"
    
    def get_service_type(self) -> str:
        if self._actual_device == "cuda":
            return "gpu"
        return "cpu"
    
    def get_service_info(self) -> dict:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–∏—Å–µ"""
        return {
            "name": self.get_service_name(),
            "model": self.model_name,
            "available": self.is_available(),
            "device": self._actual_device or self.device,
            "torch_dtype": self.torch_dtype,
            "max_new_tokens": self.max_new_tokens,
            "prompts_available": list(self.PROMPTS.keys()),
        }
    
    def unload_model(self):
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
        if self._model is not None:
            del self._model
            self._model = None
        if self._processor is not None:
            del self._processor
            self._processor = None
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("üóëÔ∏è –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏")
    
    def __repr__(self) -> str:
        status = "‚úÖ" if self.is_available() else "‚ùå"
        return f"QwenVLService({status}, model={self.model_name})"


def is_qwen_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Qwen VL"""
    return TORCH_AVAILABLE and TRANSFORMERS_AVAILABLE and PIL_AVAILABLE
