#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (—Å–µ–∫—Ü–∏—è 3) –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π (—Å–µ–∫—Ü–∏—è 4) –∏–∑ 410 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ë–ù–î.

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: output3/full_run_latest/*/full_content.md
–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
  - output3/definitions.json   (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ 3)
  - output3/abbreviations.json (—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ 4)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 scripts/utils/extract_definitions.py
"""

import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path


# ‚îÄ‚îÄ‚îÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

BASE_DIR = Path(__file__).resolve().parent.parent.parent  # –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
INPUT_DIR = BASE_DIR / "output3" / "full_run_latest"
OUTPUT_DIR = BASE_DIR / "output3"

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞—á–∞–ª–∞ —Å–µ–∫—Ü–∏–∏ 3 (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
# –ü–æ–∫—Ä—ã–≤–∞—é—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã: "# 3 –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø", "# 3 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "# 3 –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø/ DEFINITIONS"
SEC3_START_PATTERNS = [
    re.compile(r'^#\s+3\s+–û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø', re.IGNORECASE),
    re.compile(r'^#\s+3\s+–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è', re.IGNORECASE),
    re.compile(r'^#\s+3\s+DEFINITIONS', re.IGNORECASE),
    re.compile(r'^#\s+3\.?\s+–û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø', re.IGNORECASE),
    re.compile(r'^#\s+3\.?\s+–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è', re.IGNORECASE),
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞—á–∞–ª–∞ —Å–µ–∫—Ü–∏–∏ 4 (—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è)
# –ü–æ–∫—Ä—ã–≤–∞—é—Ç: "# 4 –û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø –ò –°–û–ö–†–ê–©–ï–ù–ò–Ø", OCR-–æ—à–∏–±–∫–∏ "–°–û–ö–†–ê–©–ï–ï–ù–ù–ò–ò–Ø–Ø"
SEC4_START_PATTERNS = [
    re.compile(r'^#\s+4\s+–û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø\s+–ò\s+–°–û–ö–†–ê–©', re.IGNORECASE),
    re.compile(r'^#\s+4\s+–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è\s+–∏\s+—Å–æ–∫—Ä–∞—â', re.IGNORECASE),
    re.compile(r'^#\s+4\s+TERMS\s+AND\s+ABBREVIATIONS', re.IGNORECASE),
    re.compile(r'^#\s+4\s+–°–û–ö–†–ê–©–ï–ù–ò–Ø', re.IGNORECASE),
    re.compile(r'^#\s+4\.?\s+–û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø', re.IGNORECASE),
    re.compile(r'^#\s+4\.?\s+–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è', re.IGNORECASE),
    re.compile(r'^#\s+4\.?\s+–°–û–ö–†–ê–©–ï–ù–ò–Ø', re.IGNORECASE),
]

# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–æ–¥—Å–µ–∫—Ü–∏–∏ 4.2 –°–û–ö–†–ê–©–ï–ù–ò–Ø (–≤–Ω—É—Ç—Ä–∏ —Å–µ–∫—Ü–∏–∏ 4)
SEC4_2_PATTERNS = [
    re.compile(r'^#+\s+4\.2\s+–°–û–ö–†–ê–©–ï–ù–ò–Ø', re.IGNORECASE),
    re.compile(r'^#+\s+4\.2\s+–°–æ–∫—Ä–∞—â–µ–Ω–∏—è', re.IGNORECASE),
    re.compile(r'^#+\s+4\.2\s+ABBREVIATIONS', re.IGNORECASE),
    re.compile(r'^#+\s+–°–û–ö–†–ê–©–ï–ù–ò–Ø\s*/\s*ABBREVIATIONS', re.IGNORECASE),
]

# –ü–∞—Ç—Ç–µ—Ä–Ω —Å–ª–µ–¥—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è (–∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–π —Å–µ–∫—Ü–∏–∏)
NEXT_SECTION_PATTERN = re.compile(r'^#\s+\d+[\.\s]')

# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –º—É—Å–æ—Ä–Ω—ã—Ö —Å—Ç—Ä–æ–∫
JUNK_PATTERNS = [
    re.compile(r'^<!--\s*–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+\d+\s*-->'),         # page breaks
    re.compile(r'^–ò–∑–º–µ–Ω–µ–Ω–∏–µ/Revision'),                    # revision marks
    re.compile(r'^\s*$'),                                  # –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    re.compile(r'^---+$'),                                 # –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏
    re.compile(r'^\|\s*-+\s*\|'),                          # markdown table separators
]

# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è/—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è: ¬´–¢–µ—Ä–º–∏–Ω ‚Äì —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞¬ª
# –¢–∏—Ä–µ –º–æ–∂–µ—Ç –±—ã—Ç—å: ‚Äì (em-dash), ‚Äî (em-dash), - (hyphen)
TERM_DASH_PATTERN = re.compile(
    r'^(.+?)\s+[‚Äì‚Äî\-]\s+(.+)',
    re.DOTALL
)

# –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: | –¢–µ—Ä–º–∏–Ω ‚Äì —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ |
TABLE_TERM_PATTERN = re.compile(
    r'^\|\s*(.+?)\s+[‚Äì‚Äî\-]\s+(.+?)\s*\|?\s*$'
)


# ‚îÄ‚îÄ‚îÄ –£—Ç–∏–ª–∏—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_doc_code(dirname: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏.
    '02_–î–ü-–ë1.004-06' -> '–î–ü-–ë1.004-06'
    """
    parts = dirname.split('_', 1)
    if len(parts) == 2:
        return parts[1]
    return dirname


def is_junk_line(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –º—É—Å–æ—Ä–Ω–æ–π."""
    for p in JUNK_PATTERNS:
        if p.match(line):
            return True
    return False


def is_document_header(line: str, doc_code: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    –ü—Ä–∏–º–µ—Ä: '–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–æ—Ä—Ç–æ–≤—ã—Ö –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –î–ü-–ë1.004-06'
    """
    if doc_code and doc_code in line and len(line.strip()) < 200:
        # –°—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª
        return True
    return False


def clean_table_line(line: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç –º–∞—Ä–∫–µ—Ä—ã markdown-—Ç–∞–±–ª–∏—Ü—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏."""
    line = line.strip()
    if line.startswith('|'):
        line = line[1:]
    if line.endswith('|'):
        line = line[:-1]
    # –£–±–∏—Ä–∞–µ–º  (bullet) –≤ –Ω–∞—á–∞–ª–µ
    line = line.strip()
    if line.startswith(''):
        line = line[1:].strip()
    return line.strip()


def is_toc_line(line: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è (TOC).
    
    –ü—Ä–∏–º–µ—Ä—ã TOC:
      # 3 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è .................................................................................................................... 8
      # 4 –û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è ......................................................................................... 10
      # 3 –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶9
    """
    # –ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ (–æ–±—ã—á–Ω—ã–µ —Ç–æ—á–∫–∏)
    if '...' in line:
        return True
    # –ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ (—Å–∏–º–≤–æ–ª ‚Ä¶)
    if '‚Ä¶' in line:
        return True
    # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—á–∏—Å–ª–æ) ‚Äî TOC –ø–∞—Ç—Ç–µ—Ä–Ω
    stripped = line.rstrip()
    if re.search(r'\s+\d{1,3}\s*$', stripped) and len(stripped) > 40:
        return True
    return False


def find_section(lines: list, start_patterns: list, start_idx: int = 0) -> int:
    """–ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–µ–∫—Ü–∏—è (–ø—Ä–æ–ø—É—Å–∫–∞—è TOC)."""
    for i in range(start_idx, len(lines)):
        line = lines[i].strip()
        for p in start_patterns:
            if p.match(line):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º TOC-–∑–∞–ø–∏—Å–∏
                if is_toc_line(line):
                    continue
                return i
    return -1


def find_section_end(lines: list, start_idx: int) -> int:
    """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω–µ—Ü —Å–µ–∫—Ü–∏–∏ ‚Äî –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π —Å–µ–∫—Ü–∏–∏ # N –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞."""
    for i in range(start_idx + 1, len(lines)):
        line = lines[i].strip()
        if NEXT_SECTION_PATTERN.match(line):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–∞ –∂–µ —Å–µ–∫—Ü–∏—è (–ø–æ–¥—Å–µ–∫—Ü–∏—è ##)
            if line.startswith('# ') and not line.startswith('## '):
                return i
    return len(lines)


def merge_continuation_lines(raw_lines: list, doc_code: str) -> list:
    """–°–∫–ª–µ–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏.
    
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∏–∑-–∑–∞ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ PDF.
    –ù—É–∂–Ω–æ —Å–∫–ª–µ–∏—Ç—å —Å—Ç—Ä–æ–∫–∏-–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –≤ –æ–¥–∏–Ω –±–ª–æ–∫.
    """
    blocks = []
    current_block = ""

    for line in raw_lines:
        line = line.rstrip()

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º—É—Å–æ—Ä
        if is_junk_line(line):
            continue
        if is_document_header(line, doc_code):
            continue
        # –ü–æ–¥—Å–µ–∫—Ü–∏–∏ (## 4.1 –û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        if line.strip().startswith('## ') or line.strip().startswith('### '):
            if current_block:
                blocks.append(current_block.strip())
                current_block = ""
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞–∫ –º–∞—Ä–∫–µ—Ä
            blocks.append("__SUBSECTION__:" + line.strip())
            continue

        stripped = line.strip()
        if not stripped:
            continue

        # –¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: | ... |
        if stripped.startswith('|'):
            cleaned = clean_table_line(stripped)
            if not cleaned or cleaned.startswith('-'):
                continue
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å –Ω–æ–≤–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            if TERM_DASH_PATTERN.match(cleaned):
                if current_block:
                    blocks.append(current_block.strip())
                current_block = cleaned
            else:
                # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
                current_block += " " + cleaned
            continue

        # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
        # –ù–æ–≤—ã–π —Ç–µ—Ä–º–∏–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã + —Ç–∏—Ä–µ
        if TERM_DASH_PATTERN.match(stripped) and not stripped[0].islower():
            if current_block:
                blocks.append(current_block.strip())
            current_block = stripped
        else:
            # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –±–ª–æ–∫–∞
            if current_block:
                current_block += " " + stripped
            else:
                current_block = stripped

    if current_block:
        blocks.append(current_block.strip())

    return blocks


def parse_term_definition(block: str) -> tuple:
    """–ü–∞—Ä—Å–∏—Ç –±–ª–æ–∫ '–¢–µ—Ä–º–∏–Ω ‚Äì –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ' –≤ –∫–æ—Ä—Ç–µ–∂ (term, definition).
    
    Returns:
        (term, definition) –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
    """
    if block.startswith("__SUBSECTION__:"):
        return None

    m = TERM_DASH_PATTERN.match(block)
    if m:
        term = m.group(1).strip()
        definition = m.group(2).strip()

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Ä–º–∏–Ω–∞
        term = re.sub(r'\s+', ' ', term)
        # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        term = term.lstrip('‚Ä¢¬∑‚ñ™‚ñ∫‚óè‚ñ†‚óã‚óÜ‚òÖ‚òÜ‚úì‚úî ')

        # –û—á–∏—Å—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definition = re.sub(r'\s+', ' ', definition)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        if len(term) < 2 or len(definition) < 15:
            return None
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (–Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
        if re.match(r'^\d+[\.\)]\s', term):
            return None
        # –¢–µ—Ä–º–∏–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–∏—Ä–µ ‚Äî –æ–±—Ä—ã–≤–æ–∫ —Å—Ç—Ä–æ–∫–∏
        if term.startswith('‚Äì') or term.startswith('‚Äî'):
            return None
        # –¢–µ—Ä–º–∏–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–æ —Å–∫–æ–±–∫–∏ ‚Äî –æ–±—Ä—ã–≤–æ–∫ –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        if term.startswith('('):
            return None
        # –¢–µ—Ä–º–∏–Ω ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ –∏–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        if re.match(r'^\d+\.?\s*$', term):
            return None
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ‚Äî –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ (—É—Ç–µ—á–∫–∞ –∏–∑ TOC)
        if re.match(r'^\d+\.?\s*$', definition):
            return None

        # OCR-–∫–∞—à–∞: –±—É–∫–≤—ã —É—Ç—Ä–æ–µ–Ω—ã/—É–¥–≤–æ–µ–Ω—ã (–∞–∞–∞, –±–±–±) –∏–ª–∏ —Å–ª–æ–≤–∞ —Å–ª–∏–ø–ª–∏—Å—å (> 25 –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –ø–æ–¥—Ä—è–¥)
        if re.search(r'([–∞-—è–ê-–Ø—ë–Å])\1{2,}', term) or re.search(r'([–∞-—è–ê-–Ø—ë–Å])\1{2,}', definition):
            return None
        if re.search(r'[–∞-—è–ê-–Ø—ë–Å]{25,}', term):
            return None
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ‚Äî –∫–∞—à–∞: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –ø—Ä–æ–±–µ–ª–æ–≤ –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        if len(definition) > 50 and definition.count(' ') / len(definition) < 0.05:
            return None

        return (term, definition)
    
    return None


def has_cyrillic(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
    return bool(re.search(r'[–∞-—è–ê-–Ø—ë–Å]', text))


def extract_en_term(en_text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω –∏–∑ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π —á–∞—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è/—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏.
    
    –ü–∞—Ç—Ç–µ—Ä–Ω—ã:
      "Aeronautical information - information obtained..."  ‚Üí "Aeronautical information"
      "Database - collection of..."                         ‚Üí "Database"
      "(International Air Transport Association)"           ‚Üí "International Air Transport Association"
      "ANI - aeronautical information"                      ‚Üí "ANI"
      "Company ‚Äì UTair Aviation, Public Joint Stock..."     ‚Üí "Company"
      "(Flight management system)"                          ‚Üí "Flight management system"
    """
    if not en_text:
        return ""

    text = en_text.strip()

    # –ï—Å–ª–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –≤ —Å–∫–æ–±–∫–∞—Ö: "(International Air Transport Association)"
    if text.startswith('(') and ')' in text:
        inner = text[1:text.index(')')]
        return inner.strip()

    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–∏—Ä–µ: " - " –∏–ª–∏ " ‚Äì " –∏–ª–∏ "‚Äì "
    for sep in [' - ', ' ‚Äì ', '‚Äì ', '‚Äî ']:
        if sep in text:
            term = text.split(sep, 1)[0].strip()
            # –ï—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã
            if 1 < len(term) < 100:
                return term

    # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–∏—Ä–µ, –Ω–æ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ‚Äî –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ –µ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω
    if len(text) < 60:
        return text.rstrip('.,;: ')

    return ""


def clean_ocr_artifacts(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç OCR-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π/—Å–æ–∫—Ä–∞—â–µ–Ω–∏–π.
    
    - –ü–∞–π–ø-—Å–∏–º–≤–æ–ª—ã | (–æ—Å—Ç–∞—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü/—á–µ–∫–±–æ–∫—Å–æ–≤)
    - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    - –û–±—Ä–µ–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ (–≤–∫–ª—é- –∞—é—â–∏–π ‚Üí –≤–∫–ª—é—á–∞—é—â–∏–π)
    """
    if not text:
        return text
    # –£–±–∏—Ä–∞–µ–º –ø–∞–π–ø-—Å–∏–º–≤–æ–ª—ã (–æ–¥–∏–Ω–æ—á–Ω—ã–µ –∏ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –≤–æ–∫—Ä—É–≥)
    text = re.sub(r'\s*\|\s*', ' ', text)
    # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å–ª–æ–≤: "–≤–∫–ª—é- –∞—é—â–∏–π" ‚Üí "–≤–∫–ª—é—á–∞—é—â–∏–π"
    text = re.sub(r'(\w)- (\w)', r'\1\2', text)
    # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã ‚Üí –æ–¥–∏–Ω
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip()


def split_ru_en(text: str) -> tuple:
    """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å–º–µ—à–∞–Ω–Ω—ã–π ru+en —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫—É—é –∏ –∞–Ω–≥–ª–∏–π—Å–∫—É—é —á–∞—Å—Ç–∏.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º: –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω (—Å–ª–æ–≤–æ) –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É —Å–∏–º–≤–æ–ª–æ–≤.
    –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (—Ü–∏—Ñ—Ä—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, —Å–∫–æ–±–∫–∏) –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫ –æ–±–æ–∏–º –ø–æ—Ç–æ–∫–∞–º.
    
    Returns:
        (ru_text, en_text) ‚Äî —Ä—É—Å—Å–∫–∞—è –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è —á–∞—Å—Ç–∏.
        –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–¥–Ω–æ—è–∑—ã—á–Ω—ã–π, –≤—Ç–æ—Ä–æ–µ –ø–æ–ª–µ –±—É–¥–µ—Ç –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π.
    """
    if not text:
        return ("", "")

    tokens = re.findall(r'\S+', text)

    ru_parts = []
    en_parts = []

    for token in tokens:
        has_cyr = bool(re.search(r'[–∞-—è–ê-–Ø—ë–Å]', token))
        has_lat = bool(re.search(r'[a-zA-Z]', token))

        if has_cyr and not has_lat:
            ru_parts.append(token)
        elif has_lat and not has_cyr:
            en_parts.append(token)
        elif has_cyr and has_lat:
            # –°–º–µ—à–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω (–Æ–¢—ç–π—Ä, SAP-—Å–∏—Å—Ç–µ–º–∞) ‚Äî –≤ –æ–±–µ —á–∞—Å—Ç–∏
            ru_parts.append(token)
            en_parts.append(token)
        else:
            # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π (—Ü–∏—Ñ—Ä—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, —Å–∫–æ–±–∫–∏) ‚Äî –≤ –æ–±–µ —á–∞—Å—Ç–∏
            if ru_parts:
                ru_parts.append(token)
            if en_parts:
                en_parts.append(token)

    ru_text = ' '.join(ru_parts).strip()
    en_text = ' '.join(en_parts).strip()

    # –û—á–∏—Å—Ç–∫–∞: —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –≤–∏—Å—è—â—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    ru_text = re.sub(r'\s+', ' ', ru_text).strip()
    en_text = re.sub(r'\s+', ' ', en_text).strip()
    # –£–±–∏—Ä–∞–µ–º –≤–∏—Å—è—â–∏–µ —Ç–∏—Ä–µ/–∑–∞–ø—è—Ç—ã–µ –Ω–∞ –∫—Ä–∞—è—Ö
    ru_text = re.sub(r'^[\s,;.\-‚Äì‚Äî]+|[\s,;\-‚Äì‚Äî]+$', '', ru_text).strip()
    en_text = re.sub(r'^[\s,;.\-‚Äì‚Äî]+|[\s,;\-‚Äì‚Äî]+$', '', en_text).strip()

    # –ï—Å–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∞—è —á–∞—Å—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è (< 20 —Å–∏–º–≤–æ–ª–æ–≤), 
    # —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø—Ä–æ—Å—Ç–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ (SAP, IATA, ISO),
    # –∞ –Ω–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
    if len(en_text) < 20:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –∫–∞–∫ —Ä—É—Å—Å–∫–∏–π, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π ‚Äî –ø—É—Å—Ç–æ–π
        cleaned_original = re.sub(r'\s+', ' ', text).strip()
        return (cleaned_original, "")

    return (ru_text, en_text)


# ‚îÄ‚îÄ‚îÄ –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_definitions_from_file(filepath: Path, dirname: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ 3 —Ñ–∞–π–ª–∞ full_content.md."""
    doc_code = extract_doc_code(dirname)

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        print(f"  [–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {filepath}: {e}")
        return []

    # –ò—â–µ–º —Å–µ–∫—Ü–∏—é 3 (find_section –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç TOC-–∑–∞–ø–∏—Å–∏)
    sec3_idx = find_section(lines, SEC3_START_PATTERNS)
    if sec3_idx == -1:
        return []

    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü —Å–µ–∫—Ü–∏–∏ 3
    sec3_end = find_section_end(lines, sec3_idx)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–µ–∫—Ü–∏–∏ 3 (–±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞)
    section_lines = [l.rstrip('\n') for l in lines[sec3_idx + 1:sec3_end]]

    # –°–∫–ª–µ–∏–≤–∞–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    blocks = merge_continuation_lines(section_lines, doc_code)

    # –ü–∞—Ä—Å–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    results = []
    for block in blocks:
        parsed = parse_term_definition(block)
        if parsed:
            term, definition = parsed
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å—Ç–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–±–µ–∑ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –≤ —Ç–µ—Ä–º–∏–Ω–µ)
            if not has_cyrillic(term):
                continue
            # –û—á–∏—Å—Ç–∫–∞ OCR-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ ru/en
            definition = clean_ocr_artifacts(definition)
            def_ru, def_en = split_ru_en(definition)
            def_ru = clean_ocr_artifacts(def_ru)
            def_en = clean_ocr_artifacts(def_en)
            term_en = extract_en_term(def_en)
            results.append({
                "term": term,
                "term_en": term_en,
                "definition_ru": def_ru,
                "definition_en": def_en,
                "source": doc_code,
                "source_dir": dirname,
            })

    return results


def extract_abbreviations_from_file(filepath: Path, dirname: str) -> list:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ 4 —Ñ–∞–π–ª–∞ full_content.md."""
    doc_code = extract_doc_code(dirname)

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except Exception as e:
        print(f"  [–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {filepath}: {e}")
        return []

    # –ò—â–µ–º —Å–µ–∫—Ü–∏—é 4 (find_section –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç TOC-–∑–∞–ø–∏—Å–∏)
    sec4_idx = find_section(lines, SEC4_START_PATTERNS)
    if sec4_idx == -1:
        return []

    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–µ—Ü —Å–µ–∫—Ü–∏–∏ 4
    sec4_end = find_section_end(lines, sec4_idx)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–µ–∫—Ü–∏–∏ 4 (–±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞)
    section_lines = [l.rstrip('\n') for l in lines[sec4_idx + 1:sec4_end]]

    # –°–∫–ª–µ–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏
    blocks = merge_continuation_lines(section_lines, doc_code)

    # –§–∏–ª—å—Ç—Ä—É–µ–º: –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Å–µ–∫—Ü–∏—è 4.1 –û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ 4.2 –°–û–ö–†–ê–©–ï–ù–ò–Ø
    # –ü–æ–¥—Å–µ–∫—Ü–∏—è 4.1 —Å–æ–¥–µ—Ä–∂–∏—Ç –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã, –Ω–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
    in_subsection_41 = False
    in_subsection_42 = False
    filtered_blocks = []

    for block in blocks:
        if block.startswith("__SUBSECTION__:"):
            header = block.split(":", 1)[1].strip().upper()
            if '4.1' in header or '–û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø' in header and '–°–û–ö–†–ê–©–ï–ù–ò–Ø' not in header:
                in_subsection_41 = True
                in_subsection_42 = False
                continue
            if '4.2' in header or '–°–û–ö–†–ê–©–ï–ù–ò–Ø' in header or 'ABBREVIATIONS' in header:
                in_subsection_41 = False
                in_subsection_42 = True
                continue
            continue

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Å–µ–∫—Ü–∏–π ‚Äî –±–µ—Ä—ë–º –≤—Å—ë
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Å–µ–∫—Ü–∏—è 4.1 ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if in_subsection_41 and not in_subsection_42:
            continue

        filtered_blocks.append(block)

    # –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Å–µ–∫—Ü–∏–π ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ blocks
    if not in_subsection_41 and not in_subsection_42:
        filtered_blocks = [b for b in blocks if not b.startswith("__SUBSECTION__:")]

    # –ü–∞—Ä—Å–∏–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
    results = []
    for block in filtered_blocks:
        if block.startswith("__SUBSECTION__:"):
            continue

        parsed = parse_term_definition(block)
        if parsed:
            abbr, expansion = parsed
            # –£–±–∏—Ä–∞–µ–º —á–∏—Å—Ç–æ –º—É—Å–æ—Ä–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            if abbr.lower() in ('–æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è', '–Ω–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è'):
                continue
            # –û—á–∏—Å—Ç–∫–∞ OCR-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ ru/en
            expansion = clean_ocr_artifacts(expansion)
            exp_ru, exp_en = split_ru_en(expansion)
            exp_ru = clean_ocr_artifacts(exp_ru)
            exp_en = clean_ocr_artifacts(exp_en)
            abbr_en = extract_en_term(exp_en)
            results.append({
                "abbreviation": abbr,
                "abbreviation_en": abbr_en,
                "expansion_ru": exp_ru,
                "expansion_en": exp_en,
                "source": doc_code,
                "source_dir": dirname,
            })

    return results


# ‚îÄ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    if not INPUT_DIR.exists():
        print(f"–û–®–ò–ë–ö–ê: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {INPUT_DIR}")
        sys.exit(1)

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–∞–ø–∫–∏
    dirs = sorted([
        d.name for d in INPUT_DIR.iterdir()
        if d.is_dir() and (d / "full_content.md").exists()
    ])

    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫ —Å full_content.md: {len(dirs)}")
    print(f"üìÅ –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {INPUT_DIR}")
    print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {OUTPUT_DIR}")
    print()

    all_definitions = []
    all_abbreviations = []
    docs_with_defs = 0
    docs_with_abbrs = 0
    docs_without_sec3 = []
    docs_without_sec4 = []

    for i, dirname in enumerate(dirs, 1):
        filepath = INPUT_DIR / dirname / "full_content.md"
        doc_code = extract_doc_code(dirname)

        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        if i % 50 == 0 or i == len(dirs):
            print(f"  [{i}/{len(dirs)}] –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        defs = extract_definitions_from_file(filepath, dirname)
        if defs:
            all_definitions.extend(defs)
            docs_with_defs += 1
        else:
            docs_without_sec3.append(dirname)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
        abbrs = extract_abbreviations_from_file(filepath, dirname)
        if abbrs:
            all_abbreviations.extend(abbrs)
            docs_with_abbrs += 1
        else:
            docs_without_sec4.append(dirname)

    # ‚îÄ‚îÄ‚îÄ –§–æ—Ä–º–∏—Ä—É–µ–º JSON ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    definitions_json = {
        "metadata": {
            "generated": now,
            "description": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ 3 –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ë–ù–î",
            "total_documents_processed": len(dirs),
            "documents_with_definitions": docs_with_defs,
            "documents_without_definitions": len(docs_without_sec3),
            "total_entries": len(all_definitions),
        },
        "entries": all_definitions,
    }

    abbreviations_json = {
        "metadata": {
            "generated": now,
            "description": "–û–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏–∑ —Å–µ–∫—Ü–∏–∏ 4 –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ë–ù–î",
            "total_documents_processed": len(dirs),
            "documents_with_abbreviations": docs_with_abbrs,
            "documents_without_abbreviations": len(docs_without_sec4),
            "total_entries": len(all_abbreviations),
        },
        "entries": all_abbreviations,
    }

    # ‚îÄ‚îÄ‚îÄ –°–æ—Ö—Ä–∞–Ω—è–µ–º ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    defs_path = OUTPUT_DIR / "definitions.json"
    abbrs_path = OUTPUT_DIR / "abbreviations.json"

    with open(defs_path, 'w', encoding='utf-8', newline='\n') as f:
        json.dump(definitions_json, f, ensure_ascii=False, indent=2)

    with open(abbrs_path, 'w', encoding='utf-8', newline='\n') as f:
        json.dump(abbreviations_json, f, ensure_ascii=False, indent=2)

    # ‚îÄ‚îÄ‚îÄ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    print()
    print("=" * 60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 60)
    print()
    print(f"üìó –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø (—Å–µ–∫—Ü–∏—è 3):")
    print(f"   –§–∞–π–ª: {defs_path}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(dirs)}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏: {docs_with_defs}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —Å–µ–∫—Ü–∏–∏ 3: {len(docs_without_sec3)}")
    print(f"   –í—Å–µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(all_definitions)}")
    print()
    print(f"üìò –°–û–ö–†–ê–©–ï–ù–ò–Ø (—Å–µ–∫—Ü–∏—è 4):")
    print(f"   –§–∞–π–ª: {abbrs_path}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(dirs)}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è–º–∏: {docs_with_abbrs}")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —Å–µ–∫—Ü–∏–∏ 4: {len(docs_without_sec4)}")
    print(f"   –í—Å–µ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(all_abbreviations)}")
    print()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
    from collections import Counter
    def_counts = Counter(d["source"] for d in all_definitions)
    abbr_counts = Counter(a["source"] for a in all_abbreviations)

    defs_with_en = sum(1 for d in all_definitions if d["definition_en"])
    defs_with_term_en = sum(1 for d in all_definitions if d["term_en"])
    abbrs_with_en = sum(1 for a in all_abbreviations if a["expansion_en"])
    abbrs_with_abbr_en = sum(1 for a in all_abbreviations if a["abbreviation_en"])

    print(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å –∞–Ω–≥–ª. –ø–µ—Ä–µ–≤–æ–¥–æ–º: {defs_with_en}")
    print(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å –∞–Ω–≥–ª. —Ç–µ—Ä–º–∏–Ω–æ–º: {defs_with_term_en}")
    print(f"   –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º: {len(all_definitions) - defs_with_en}")
    print()
    print(f"   –°–æ–∫—Ä–∞—â–µ–Ω–∏–π —Å –∞–Ω–≥–ª. –ø–µ—Ä–µ–≤–æ–¥–æ–º: {abbrs_with_en}")
    print(f"   –°–æ–∫—Ä–∞—â–µ–Ω–∏–π —Å –∞–Ω–≥–ª. —Ç–µ—Ä–º–∏–Ω–æ–º: {abbrs_with_abbr_en}")
    print(f"   –°–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º: {len(all_abbreviations) - abbrs_with_en}")
    print()

    print("üìà –¢–æ–ø-10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º:")
    for doc, count in def_counts.most_common(10):
        print(f"   {doc}: {count}")
    print()

    print("üìà –¢–æ–ø-10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è–º:")
    for doc, count in abbr_counts.most_common(10):
        print(f"   {doc}: {count}")
    print()

    # –ü—Ä–∏–º–µ—Ä—ã –±–µ–∑ —Å–µ–∫—Ü–∏–∏ 3
    if docs_without_sec3:
        print(f"‚ö†Ô∏è  –ü—Ä–∏–º–µ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —Å–µ–∫—Ü–∏–∏ 3 (–ø–µ—Ä–≤—ã–µ 10):")
        for d in docs_without_sec3[:10]:
            print(f"   {d}")
        print()

    print("‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
