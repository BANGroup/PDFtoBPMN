#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π.

–ò–∑–≤–ª–µ–∫–∞–µ—Ç:
1. –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (native + OCR)
2. –°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ –ø–æ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ –ø—É–Ω–∫—Ç–æ–≤
3. –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∂–¥–æ–º—É —É–∑–ª—É –¥–µ—Ä–µ–≤–∞
4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π MD —Å —Ç–µ–∫—Å—Ç–æ–º –≤—Å–µ—Ö –ø—É–Ω–∫—Ç–æ–≤
"""

import sys
import json
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from scripts.pdf_to_context.pipeline import PDFToContextPipeline
from scripts.document_graph.hierarchy_builder import (
    DocumentTree,
    SectionNode,
    build_hierarchy,
    flatten_tree,
    parse_section_number,
    export_tree_json,
)
from scripts.document_graph.hybrid_parser import (
    parse_document,
    extract_doc_code,
)

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä (–ª—É—á—à–µ –ø–æ—Ä—è–¥–æ–∫ —Ç–µ–∫—Å—Ç–∞)
try:
    from scripts.document_graph.pdfplumber_extractor import extract_text_pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False


@dataclass
class FullParseResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    doc_code: str
    source: str  # "pdf" –∏–ª–∏ "docx"
    full_markdown: str  # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
    tree: DocumentTree  # –ò–µ—Ä–∞—Ä—Ö–∏—è —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
    stats: Dict


def clean_markdown(markdown: str) -> str:
    """
    –û—á–∏—Å—Ç–∫–∞ markdown –æ—Ç –º—É—Å–æ—Ä–∞.
    
    –£–¥–∞–ª—è–µ—Ç:
    - –ö–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã (–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è... –°—Ç—Ä. X –∏–∑ Y)
    - –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–∞—Ö
    - –ú—É—Å–æ—Ä –æ—Ç —Ç–∏—Ç—É–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞ —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π)
    - OCR –º—É—Å–æ—Ä –∏–∑ —Å—Ö–µ–º –∏ —Ç–∞–±–ª–∏—Ü
    - –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥—Ä—è–¥
    """
    lines = markdown.split('\n')
    cleaned = []
    skip_change_log = False
    heading_pattern = r'^(?:#{1,6}\s+|\d+(?:\.\d+)*\s+)'
    change_log_pattern = r'–ª–∏—Å—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≤–Ω–µ—Å–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π'
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    garbage_patterns = [
        # –ö–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã
        r'^–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è.*–°—Ç—Ä\.\s*\d+\s*–∏–∑\s*\d+',
        r'^–°—Ç—Ä\.\s*\d+\s*–∏–∑\s*\d+',
        r'^–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è',
        r'^–û—Å–Ω–æ–≤–∞–Ω–∏–µ:?',
        r'^#\s+.*–î–ü-[–ê-–Ø–∞-—è0-9.-]+\s*\{#',  # –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–µ
        r'^#\s+[–ê-–Ø–∞-—è\s]+\s+–î–ü-[–ê-–Ø–∞-—è0-9.-]+\s*\{#',
        
        # –ú—É—Å–æ—Ä –æ—Ç —Ç–∏—Ç—É–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–≤–µ–∫—Ç–æ—Ä–Ω–∞—è –≥—Ä–∞—Ñ–∏–∫–∞ —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π)
        r'^#{1,6}\s*TIYBIII',  # "–ü–£–ë–õ–ò–ß–ù–û–ï" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*AKUI4OHEP',  # "–ê–ö–¶–ò–û–ù–ï–†–ù–û–ï" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*YTBEPx',  # "–£–¢–í–ï–†–ñ–î–ï–ù–ê" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*CI4CTEMA',  # "–°–ò–°–¢–ï–ú–ê" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*AII-\d',  # "–î–ü-" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*<<An\s*u',  # "–ê–≤–∏–∞–∫–æ–º–ø–∞–Ω–∏—è" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*flara',  # "–î–∞—Ç–∞" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*lpr4Ka3oM',  # "–ø—Ä–∏–∫–∞–∑–æ–º" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*reHepanbHofo',  # "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*Ar\{peKTopa',  # "–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*rlaprepnbre',  # "–ß–∞—Ä—Ç–µ—Ä–Ω—ã–µ" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*peficrr',  # "—Ä–µ–π—Å—ã" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*Xanrsr-M',  # "–•–∞–Ω—Ç—ã-–ú–∞–Ω—Å–∏–π—Å–∫" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^#{1,6}\s*OEIUECTB',  # "–û–ë–©–ï–°–¢–í–û" —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'^r\.\s*X[a-z]',  # "–≥. –•–∞–Ω—Ç—ã..." —Å –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        r'TIYBIII',  # –í –ª—é–±–æ–º –º–µ—Å—Ç–µ —Å—Ç—Ä–æ–∫–∏
        r'AKUI4OHEP',
        r'OEIUECTB\s*O',
        
        # OCR –º—É—Å–æ—Ä –∏–∑ —Å—Ö–µ–º –∏ —Ç–∞–±–ª–∏—Ü
        r'^#\s+(–ù–ï\s+–í–´–ü–û–õ|–î–ê\s+–ù–ï–¢|–î–ï–ô–°–¢–í–ò–ô|–ü–†–û–î–ê–ñ–ê|–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å|–ü–†–ò–ú–ï–ß–ê–ù|–†–ï–ê–õ–ò–ó–ê–¶–ò–Ø|–ö–û–ú–ü–û–ù–û–í–ö–ê|–¢–ò–ü\s+–í–°|–î–ê–¢–ê/–í–†–ï–ú–Ø|–ú–ê–†–®–†–£–¢)',
        r'^#\s+–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ô\s+–î–ò–†–ï–ö–¢–û–†–ê–¢',
        
        # OCR –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (—Å–º–µ—à–∞–Ω–Ω–∞—è –ª–∞—Ç–∏–Ω–∏—Ü–∞/–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ - –ø—Ä–∏–∑–Ω–∞–∫ –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏)
        r'^[A-Z]{3,}[–∞-—è–ê-–Ø]',  # –õ–∞—Ç–∏–Ω–∏—Ü–∞ –ø–æ—Ç–æ–º –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
        r'^[–∞-—è–ê-–Ø]+[A-Z]{3,}',  # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ –ø–æ—Ç–æ–º –ª–∞—Ç–∏–Ω–∏—Ü–∞ (–µ—Å–ª–∏ –Ω–µ email)
        
        # –ö–æ—Ä–æ—Ç–∫–∏–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        r'^#?\s*[–∞-—è–ê-–Ø]{1,3}$',  # 1-3 –±—É–∫–≤—ã
        r'^#?\s*[–Ω–Ω—ã—Ö|—ã—Ö|—Ö|‚Ññ]+$',  # –û–±—Ä–µ–∑–∫–∏ –æ—Ç —Ç–∞–±–ª–∏—Ü
        r'^#?\s*–Ω–∏–µ$',  # "–ò–∑–º–µ–Ω–µ–Ω–∏–µ" –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–µ
        r'^#?\s*–ò–∑–º–µ–Ω–µ$',
        
        # Email –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
        r'^#\s+[–∞-—è–ê-–Ø\w.]+@',
        
        # –¢–µ–ª–µ—Ñ–æ–Ω—ã –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        r'^#?\s*\(\d+\)\s*\d+',
    ]
    
    for line in lines:
        skip = False
        line_stripped = line.strip()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–µ—Å—å —Ä–∞–∑–¥–µ–ª "–õ–ò–°–¢ –†–ï–ì–ò–°–¢–†–ê–¶–ò–ò –í–ù–ï–°–ï–ù–ò–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô"
        if skip_change_log:
            if re.match(heading_pattern, line_stripped, re.IGNORECASE):
                skip_change_log = False
            else:
                continue
        
        if re.search(change_log_pattern, line_stripped, re.IGNORECASE):
            skip_change_log = True
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for pattern in garbage_patterns:
            if re.match(pattern, line_stripped, re.IGNORECASE):
                skip = True
                break
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Å—Ç—Ä–æ–∫–∏ —Å –≤—ã—Å–æ–∫–∏–º % –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –ª–∞—Ç–∏–Ω–∏—Ü—ã
        if not skip and line_stripped:
            # –°—á–∏—Ç–∞–µ–º —Å–∏–º–≤–æ–ª—ã
            latin_chars = len(re.findall(r'[a-zA-Z]', line_stripped))
            cyrillic_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', line_stripped))
            total_alpha = latin_chars + cyrillic_chars
            
            # –ï—Å–ª–∏ > 50% –ª–∞—Ç–∏–Ω–∏—Ü—ã –∏ –ø—Ä–∏ —ç—Ç–æ–º –µ—Å—Ç—å –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã —Ä—è–¥–æ–º —Å –ª–∞—Ç–∏–Ω–∏—Ü–µ–π
            # (–ø—Ä–∏–∑–Ω–∞–∫ –±–∏—Ç–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏) - —ç—Ç–æ –º—É—Å–æ—Ä
            if total_alpha > 10 and latin_chars > total_alpha * 0.6:
                # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: –∫–æ–¥—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, email, –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
                if not re.search(
                    r'(–î–ü-|–†–ö|–°–¢–û|–ö–î-|–†–î-|–ò–û–¢-|'
                    r'ISO|IATA|ICAO|DCS|PNL|MVT|LDM|APIS|'
                    r'EASA|FAA|MEL|CDL|SB|AD|AMP|MRB|RVSM|ETOPS|'
                    r'MMEL|MPD|CMM|IPC|AMM|TSM|FIM|WDM|SRM|'
                    r'AMOS|SAP|ERP|CRM|SMS|QMS|EFB|OCC|AOC|'
                    r'NOTAM|SIGMET|METAR|TAF|RVR|ILS|VOR|NDB|DME|'
                    r'PIC|SIC|FE|LAE|TRE|TRI|SFI|SFE|'
                    r'Boeing|Airbus|ATR|Bombardier|Embraer|'
                    r'B737|B767|A320|CRJ|DHC|'
                    r'UTC|GMT|MSK|'
                    r'@|http|www\.)',
                    line_stripped
                ):
                    skip = True
        
        if not skip:
            cleaned.append(line)
    
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    result = []
    prev_empty = False
    for line in cleaned:
        is_empty = not line.strip()
        if is_empty and prev_empty:
            continue
        result.append(line)
        prev_empty = is_empty
    
    return '\n'.join(result)


def extract_sections_from_markdown(markdown: str) -> Dict[str, str]:
    """
    –†–∞–∑–±–∏—Ç—å Markdown –Ω–∞ —Å–µ–∫—Ü–∏–∏ –ø–æ –Ω—É–º–µ—Ä–∞—Ü–∏–∏.
    
    –ò—â–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞:
    ## 5.1 –ó–∞–≥–æ–ª–æ–≤–æ–∫
    –∏–ª–∏
    **5.1** –ó–∞–≥–æ–ª–æ–≤–æ–∫
    –∏–ª–∏
    5.1 –ó–∞–≥–æ–ª–æ–≤–æ–∫ (–≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏)
    
    Returns:
        Dict: {–Ω–æ–º–µ—Ä_–ø—É–Ω–∫—Ç–∞: –∫–æ–Ω—Ç–µ–Ω—Ç}
    """
    # –°–Ω–∞—á–∞–ª–∞ —á–∏—Å—Ç–∏–º –æ—Ç –º—É—Å–æ—Ä–∞
    markdown = clean_markdown(markdown)
    
    sections = {}
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
    # –ò—â–µ–º: —á–∏—Å–ª–æ.—á–∏—Å–ª–æ (–∏ —Ç–∞–∫ –¥–∞–ª–µ–µ) –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
    section_pattern = r'^(?:#{1,6}\s+)?(?:\*\*)?(\d+(?:\.\d+)*)\**\s+(.+?)$'
    
    lines = markdown.split('\n')
    current_num = None
    current_content = []
    
    for line in lines:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –Ω–æ–≤–∞—è —Å–µ–∫—Ü–∏—è?
        match = re.match(section_pattern, line)
        if match:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
            if current_num and current_content:
                # –í–∫–ª—é—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç
                sections[current_num] = '\n'.join(current_content).strip()
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
            current_num = match.group(1)
            current_content = [line]
        else:
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â—É—é —Å–µ–∫—Ü–∏—é
            current_content.append(line)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
    if current_num and current_content:
        sections[current_num] = '\n'.join(current_content).strip()
    
    return sections


def assign_content_to_tree(tree: DocumentTree, sections: Dict[str, str]) -> None:
    """
    –ü—Ä–∏—Å–≤–æ–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —É–∑–ª–∞–º –¥–µ—Ä–µ–≤–∞.
    
    Args:
        tree: –î–µ—Ä–µ–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        sections: –°–ª–æ–≤–∞—Ä—å {–Ω–æ–º–µ—Ä: –∫–æ–Ω—Ç–µ–Ω—Ç}
    """
    nodes = flatten_tree(tree.root)
    assigned = 0
    
    for node in nodes:
        if node.num in sections:
            node.content = sections[node.num]
            assigned += 1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º is_actionable
            node.is_actionable = _check_actionable(node.content)
            if node.is_actionable:
                tree.actionable_sections += 1
    
    print(f"   üìù –ü—Ä–∏—Å–≤–æ–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç: {assigned}/{len(nodes)} —É–∑–ª–æ–≤")


def _check_actionable(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—É–Ω–∫—Ç –¥–µ–π—Å—Ç–≤–∏–µ"""
    action_keywords = [
        r'–Ω–µ—Å–µ—Ç\s+–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å',
        r'–æ–±—è–∑–∞–Ω[—ã–∞]?\b',
        r'–¥–æ–ª–∂–µ–Ω\b',
        r'–¥–æ–ª–∂–Ω—ã\b',
        r'–≤—ã–ø–æ–ª–Ω—è–µ—Ç',
        r'–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç',
        r'–ø—Ä–æ–≤–æ–¥–∏—Ç',
        r'–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç',
        r'–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç',
        r'—Å–æ–≥–ª–∞—Å–æ–≤—ã–≤–∞–µ—Ç',
        r'—É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç',
        r'–Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç',
        r'–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç',
        r'–∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç',
        r'–Ω–µ—Å—ë—Ç\s+–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å',
    ]
    
    content_lower = content.lower()
    for keyword in action_keywords:
        if re.search(keyword, content_lower):
            return True
    return False


def parse_document_full(
    pdf_path: str,
    enable_ocr: bool = True,
    ocr_base_url: str = "http://localhost:8000",
    use_pdfplumber: bool = False
) -> FullParseResult:
    """
    –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    
    1. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ PDFToContextPipeline –∏–ª–∏ pdfplumber
    2. –°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    3. –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞–∂–¥–æ–º—É —É–∑–ª—É
    
    Args:
        pdf_path: –ü—É—Ç—å –∫ PDF
        enable_ocr: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OCR –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫
        ocr_base_url: URL OCR —Å–µ—Ä–≤–∏—Å–∞
        use_pdfplumber: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pdfplumber (–ª—É—á—à–∏–π –ø–æ—Ä—è–¥–æ–∫ —Ç–µ–∫—Å—Ç–∞)
    
    Returns:
        FullParseResult
    """
    pdf_path = Path(pdf_path)
    doc_code = extract_doc_code(str(pdf_path))
    
    print(f"\n{'='*60}")
    print(f"üìÑ –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: {pdf_path.name}")
    print(f"{'='*60}")
    
    # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    print("\nüîç –≠—Ç–∞–ø 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
    
    if use_pdfplumber and PDFPLUMBER_AVAILABLE:
        # pdfplumber - –ª—É—á—à–µ –ø–æ—Ä—è–¥–æ–∫ —Ç–µ–∫—Å—Ç–∞, OCR —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–∏—Ç—É–ª—å–Ω–æ–π
        print("   üìò –ò—Å–ø–æ–ª—å–∑—É–µ–º pdfplumber (–ª—É—á—à–∏–π –ø–æ—Ä—è–¥–æ–∫ —Ç–µ–∫—Å—Ç–∞)")
        if enable_ocr:
            print("   üßæ OCR –¥–ª—è —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: –≤–∫–ª—é—á–µ–Ω")
        else:
            print("   üßæ OCR –¥–ª—è —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: –æ—Ç–∫–ª—é—á–µ–Ω")
        full_markdown = extract_text_pdfplumber(
            str(pdf_path),
            ocr_title=enable_ocr,
            ocr_url=f"{ocr_base_url}/ocr/figure",
            ocr_graphics=enable_ocr,
            ocr_base_url=ocr_base_url
        )
    else:
        # PDFToContextPipeline - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OCR
        if use_pdfplumber and not PDFPLUMBER_AVAILABLE:
            print("   ‚ö†Ô∏è pdfplumber –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º PyMuPDF")
        
        pipeline = PDFToContextPipeline(
            ocr_base_url=ocr_base_url,
            enable_ocr=enable_ocr,
            extract_images=True,
            extract_drawings=True,
            extract_tables=True,
            include_frontmatter=False,  # –ë–µ–∑ frontmatter –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            include_toc=False,  # –ë–µ–∑ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
        )
        
        full_markdown = pipeline.process(str(pdf_path))
    
    print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(full_markdown)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # 2. –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á–µ—Ä–µ–∑ hybrid_parser (–¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
    print("\nüèóÔ∏è –≠—Ç–∞–ø 2: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏...")
    
    parse_result = parse_document(str(pdf_path))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ –Ω–æ–º–µ—Ä—É –ø—É–Ω–∫—Ç–∞ –ü–ï–†–ï–î –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –¥–µ—Ä–µ–≤–∞
    headings_list = [{"text": h.text, "level": h.level} for h in parse_result.headings]
    
    def heading_sort_key(h):
        text = h.get("text", "")
        num, level, title, stype = parse_section_number(text)
        if not num:
            return ((0,), text)  # –°–ª—É–∂–µ–±–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–æ - tuple –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        return (_sort_key_for_section(num), text)
    
    sorted_headings = sorted(headings_list, key=heading_sort_key)
    
    # –°—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –∏–∑ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    tree = build_hierarchy(
        headings=sorted_headings,
        doc_code=doc_code,
        source=parse_result.source
    )
    
    print(f"   ‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω–æ –¥–µ—Ä–µ–≤–æ: {tree.total_sections} —É–∑–ª–æ–≤, –≥–ª—É–±–∏–Ω–∞ {tree.max_depth}")
    
    # 3. –†–∞–∑–±–∏–≤–∞–µ–º markdown –Ω–∞ —Å–µ–∫—Ü–∏–∏
    print("\nüìã –≠—Ç–∞–ø 3: –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–∫—Ü–∏–∏...")
    
    sections = extract_sections_from_markdown(full_markdown)
    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(sections)} —Å–µ–∫—Ü–∏–π –ø–æ –Ω—É–º–µ—Ä–∞—Ü–∏–∏")
    
    # 4. –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —É–∑–ª–∞–º
    print("\nüîó –≠—Ç–∞–ø 4: –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
    
    assign_content_to_tree(tree, sections)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "total_chars": len(full_markdown),
        "total_sections": tree.total_sections,
        "sections_with_content": sum(1 for n in flatten_tree(tree.root) if n.content),
        "actionable_sections": tree.actionable_sections,
        "max_depth": tree.max_depth,
    }
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –°–∏–º–≤–æ–ª–æ–≤: {stats['total_chars']:,}")
    print(f"   –£–∑–ª–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ: {stats['total_sections']}")
    print(f"   –° –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º: {stats['sections_with_content']}")
    print(f"   Actionable: {stats['actionable_sections']}")
    
    return FullParseResult(
        doc_code=doc_code,
        source=parse_result.source,
        full_markdown=full_markdown,
        tree=tree,
        stats=stats
    )


def _sort_key_for_section(num: str) -> Tuple:
    """
    –ö–ª—é—á —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è –Ω–æ–º–µ—Ä–∞ —Å–µ–∫—Ü–∏–∏.
    
    "5.1.2" -> (5, 1, 2)
    "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ 1" -> (9999, 1)  # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤ –∫–æ–Ω–µ—Ü
    "" -> (0,)  # –°–ª—É–∂–µ–±–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–æ
    """
    if not num:
        return (0,)
    
    # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è - –≤ –∫–æ–Ω–µ—Ü
    if num.lower().startswith('–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ'):
        match = re.search(r'(\d+)', num)
        if match:
            return (9999, int(match.group(1)))
        return (9999, 0)
    
    # –û–±—ã—á–Ω–∞—è –Ω—É–º–µ—Ä–∞—Ü–∏—è
    parts = []
    for part in num.split('.'):
        try:
            parts.append(int(part))
        except ValueError:
            parts.append(0)
    return tuple(parts) if parts else (0,)


def generate_full_structure_md(result: FullParseResult) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ MD —Å –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º.
    –°–µ–∫—Ü–∏–∏ —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –ø–æ –Ω–æ–º–µ—Ä—É –ø—É–Ω–∫—Ç–∞.
    """
    lines = [
        f"# {result.doc_code}",
        "",
        f"**–ò—Å—Ç–æ—á–Ω–∏–∫:** {result.source.upper()}",
        f"**–†–∞–∑–¥–µ–ª–æ–≤:** {result.tree.total_sections}",
        f"**–° –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º:** {result.stats['sections_with_content']}",
        f"**Actionable:** {result.stats['actionable_sections']}",
        f"**–ú–∞–∫—Å. –≥–ª—É–±–∏–Ω–∞:** {result.tree.max_depth}",
        "",
        "---",
        "",
    ]
    
    def render_node(node: SectionNode, level: int = 1):
        if node.id == "root":
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–µ—Ç–µ–π –ø–æ –Ω–æ–º–µ—Ä—É
            sorted_children = sorted(node.children, key=lambda n: _sort_key_for_section(n.num))
            for child in sorted_children:
                render_node(child, 1)
            return
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π
        header_level = min(level + 1, 6)  # –ú–∞–∫—Å–∏–º—É–º h6
        num_part = f"{node.num} " if node.num else ""
        marker = "üìå " if node.is_actionable else ""
        
        lines.append(f"{'#' * header_level} {marker}{num_part}{node.title}")
        lines.append("")
        
        # –ö–æ–Ω—Ç–µ–Ω—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if node.content:
            # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = node.content
            # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –æ–Ω–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            first_line_match = re.match(r'^(?:#{1,6}\s+)?(?:\*\*)?[\d.]+\**\s+.+$', content.split('\n')[0])
            if first_line_match:
                content = '\n'.join(content.split('\n')[1:]).strip()
            
            if content:
                lines.append(content)
                lines.append("")
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–ª—è –¥–µ—Ç–µ–π (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö)
        sorted_children = sorted(node.children, key=lambda n: _sort_key_for_section(n.num))
        for child in sorted_children:
            render_node(child, level + 1)
    
    render_node(result.tree.root)
    
    return '\n'.join(lines)


def process_documents(
    pdf_paths: List[str],
    output_dir: Path,
    enable_ocr: bool = True,
    ocr_base_url: str = "http://localhost:8000",
    use_pdfplumber: bool = False
) -> None:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    extractor = "pdfplumber" if use_pdfplumber else "PyMuPDF"
    
    print(f"\n{'='*60}")
    print(f"üöÄ –ü–û–õ–ù–´–ô –ü–ê–†–°–ò–ù–ì: {len(pdf_paths)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä: {extractor}")
    if use_pdfplumber:
        print(f"   OCR —Ç–∏—Ç—É–ª—å–Ω–æ–π: {'‚úÖ –í–∫–ª—é—á–µ–Ω' if enable_ocr else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}")
    else:
        print(f"   OCR: {'‚úÖ –í–∫–ª—é—á–µ–Ω' if enable_ocr else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}")
    print(f"   Output: {output_dir}")
    print(f"{'='*60}")
    
    results = []
    
    for i, pdf_path in enumerate(pdf_paths, 1):
        print(f"\n[{i}/{len(pdf_paths)}] ", end="")
        
        try:
            result = parse_document_full(
                pdf_path,
                enable_ocr=enable_ocr,
                ocr_base_url=ocr_base_url,
                use_pdfplumber=use_pdfplumber
            )
            results.append(result)
            
            # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_dir = output_dir / f"{i:02d}_{result.doc_code}"
            doc_dir.mkdir(exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π markdown (–æ—á–∏—â–µ–Ω–Ω—ã–π)
            cleaned_markdown = clean_markdown(result.full_markdown)
            with open(doc_dir / "full_content.md", "w", encoding="utf-8") as f:
                f.write(cleaned_markdown)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π markdown
            structure_md = generate_full_structure_md(result)
            with open(doc_dir / "structure.md", "w", encoding="utf-8") as f:
                f.write(structure_md)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ä–µ–≤–æ –≤ JSON
            export_tree_json(result.tree, doc_dir / "structure_tree.json")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            with open(doc_dir / "stats.json", "w", encoding="utf-8") as f:
                json.dump(result.stats, f, ensure_ascii=False, indent=2)
            
            print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {doc_dir.name}/")
            
        except Exception as e:
            print(f"\n   ‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n{'='*60}")
    print(f"‚úÖ –ó–ê–í–ï–†–®–ï–ù–û: {len(results)}/{len(pdf_paths)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"{'='*60}")


def find_test_documents(input_dir: Path, limit: int = 8, all_pdfs: bool = False) -> List[Path]:
    """–ù–∞–π—Ç–∏ PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    pdfs = []
    
    for pdf_dir in sorted(input_dir.glob("**/pdf/*")):
        if pdf_dir.is_dir():
            for pdf_file in pdf_dir.glob("*.pdf"):
                if not all_pdfs and "–≠—Ç–∞–ª–æ–Ω –¥–ª—è –ø–µ—á–∞—Ç–∏" not in pdf_file.name:
                    continue
                pdfs.append(pdf_file)
                if limit > 0 and len(pdfs) >= limit:
                    return pdfs
    
    return pdfs


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π")
    parser.add_argument("--limit", type=int, default=8, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (0 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)")
    parser.add_argument("--all-pdfs", action="store_true", help="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—Å–µ PDF, –Ω–µ —Ç–æ–ª—å–∫–æ '–≠—Ç–∞–ª–æ–Ω –¥–ª—è –ø–µ—á–∞—Ç–∏'")
    parser.add_argument("--no-ocr", action="store_true", help="–û—Ç–∫–ª—é—á–∏—Ç—å OCR")
    parser.add_argument("--pdfplumber", action="store_true", 
                       help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pdfplumber (–ª—É—á—à–∏–π –ø–æ—Ä—è–¥–æ–∫ —Ç–µ–∫—Å—Ç–∞, OCR —Ç–æ–ª—å–∫–æ —Ç–∏—Ç—É–ª—å–Ω–æ–π)")
    parser.add_argument("--input", type=str, default="/home/budnik_an/Obligations/input2",
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    parser.add_argument("--output", type=str, default="/home/budnik_an/Obligations/output3/full_parse",
                       help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    args = parser.parse_args()
    
    # –ù–∞—Ö–æ–¥–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    pdfs = find_test_documents(Path(args.input), args.limit, all_pdfs=args.all_pdfs)
    
    if not pdfs:
        print("‚ùå PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        sys.exit(1)
    
    print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdfs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    for pdf in pdfs:
        print(f"   - {pdf.name}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    process_documents(
        [str(p) for p in pdfs],
        Path(args.output),
        enable_ocr=not args.no_ocr,
        use_pdfplumber=args.pdfplumber
    )
