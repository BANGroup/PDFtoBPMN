#!/usr/bin/env python3
"""
Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ° Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
"""

import sys
import shutil
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from scripts.document_graph.hybrid_parser import (
    parse_document,
    format_parse_report,
)
from scripts.document_graph.hierarchy_builder import export_tree_json
from scripts.document_graph.test_hybrid_parser import find_test_documents


def export_test_results(base_dir: Path, output_dir: Path, count: int = 8):
    """
    Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ
    
    Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° output_dir:
    â”œâ”€â”€ sources/           # Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ PDF Ñ„Ğ°Ğ¹Ğ»Ñ‹ (ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¿Ğ¸Ğ¸)
    â”‚   â”œâ”€â”€ 01_ĞšĞ”-Ğ”ĞŸ-Ğœ1.046-02.pdf
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ results/           # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñƒ
    â”‚   â”œâ”€â”€ 01_ĞšĞ”-Ğ”ĞŸ-Ğœ1.046-02/
    â”‚   â”‚   â”œâ”€â”€ parse_result.json
    â”‚   â”‚   â””â”€â”€ report.txt
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ reports/           # Ğ¡Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹
    â”‚   â”œâ”€â”€ summary.txt
    â”‚   â””â”€â”€ statistics.json
    â””â”€â”€ README.md          # ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
    """
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    sources_dir = output_dir / "sources"
    results_dir = output_dir / "results"
    reports_dir = output_dir / "reports"
    
    for d in [sources_dir, results_dir, reports_dir]:
        d.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² {output_dir}")
    print()
    
    # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    test_docs = find_test_documents(base_dir, count)
    
    if not test_docs:
        print("âŒ Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹")
        return
    
    print(f"âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(test_docs)} Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²")
    
    # DOCX Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ
    docx_dir = base_dir / "docx"
    docx_base = str(docx_dir) if docx_dir.exists() else None
    
    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    stats = {
        "timestamp": datetime.now().isoformat(),
        "total_documents": len(test_docs),
        "docx_used": 0,
        "pdf_used": 0,
        "filter_stats": {
            "total_blocks": 0,
            "after_filtering": 0,
            "by_repeat": 0,
            "by_blacklist": 0,
            "by_pattern": 0,
        },
        "documents": []
    }
    
    all_reports = []
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
    for i, pdf_path in enumerate(test_docs, 1):
        doc_code = pdf_path.stem.split()[0].replace("(", "").replace(")", "")
        prefix = f"{i:02d}_{doc_code}"
        
        print(f"\n[{i}/{len(test_docs)}] {doc_code}...")
        
        # 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ ÑĞ¸Ğ¼Ğ»Ğ¸Ğ½Ğº Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº
        source_link = sources_dir / f"{prefix}.pdf"
        if source_link.exists():
            source_link.unlink()
        source_link.symlink_to(pdf_path.resolve())
        
        # 2. ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚
        try:
            result = parse_document(str(pdf_path), docx_base)
            
            # 3. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
            result_dir = results_dir / prefix
            result_dir.mkdir(exist_ok=True)
            
            # 3.1 ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ PDF Ğ² Ğ¿Ğ°Ğ¿ĞºÑƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°
            local_pdf = result_dir / f"source.pdf"
            if not local_pdf.exists():
                shutil.copy2(pdf_path, local_pdf)
            
            # 3.2 ĞšĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ DOCX ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if result.docx_path:
                docx_src = Path(result.docx_path)
                if docx_src.exists():
                    local_docx = result_dir / f"source.docx"
                    if not local_docx.exists():
                        shutil.copy2(docx_src, local_docx)
            
            # 4. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ JSON Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼
            result_json = {
                "doc_code": result.doc_code,
                "source": result.source,
                "headings_count": len(result.headings),
                "headings": [{"text": h.text, "level": h.level} for h in result.headings],
                "pdf_path": str(pdf_path),
                "docx_path": result.docx_path or None,
                "validation": {
                    "is_valid": result.validation.is_valid if result.validation else None,
                    "details": result.validation.details if result.validation else None,
                } if result.validation else None,
                "filter_report": result.filter_report,
            }
            
            with open(result_dir / "parse_result.json", "w", encoding="utf-8") as f:
                json.dump(result_json, f, ensure_ascii=False, indent=2)
            
            # 4.1 Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ
            if result.structure_tree:
                export_tree_json(result.structure_tree, result_dir / "structure_tree.json")
            
            # 5. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
            report = format_parse_report(result)
            with open(result_dir / "report.txt", "w", encoding="utf-8") as f:
                f.write(report)
            
            all_reports.append(report)
            
            # 6. Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ MD Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° (ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°)
            md_lines = [
                f"# {result.doc_code or doc_code}",
                "",
                f"**Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº:** {result.source.upper()}",
                f"**Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ²:** {len(result.headings)}",
                "",
            ]
            
            if result.validation:
                md_lines.extend([
                    "## Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ DOCX",
                    "",
                    f"- Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: {'âœ… ĞĞºÑ‚ÑƒĞ°Ğ»ĞµĞ½' if result.validation.is_valid else 'âŒ ĞĞµ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ĞµĞ½'}",
                    f"- Ğ”ĞµÑ‚Ğ°Ğ»Ğ¸: {result.validation.details}",
                    "",
                ])
            
            if result.filter_report:
                fr = result.filter_report
                md_lines.extend([
                    "## Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ",
                    "",
                    f"| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ |",
                    f"|---------|----------|",
                    f"| Ğ‘Ğ»Ğ¾ĞºĞ¾Ğ² Ğ´Ğ¾ | {fr.get('total_blocks', 0)} |",
                    f"| Ğ‘Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ | {fr.get('after_filtering', 0)} |",
                    f"| ĞŸĞ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼ | {len(fr.get('by_repeat', []))} |",
                    f"| ĞŸĞ¾ blacklist | {len(fr.get('by_blacklist', []))} |",
                    f"| ĞŸĞ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼ | {len(fr.get('by_pattern', []))} |",
                    "",
                ])
            
            # Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°
            if result.structure_tree:
                tree = result.structure_tree
                md_lines.extend([
                    "## Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°",
                    "",
                    f"| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ |",
                    f"|---------|----------|",
                    f"| Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¾Ğ² | {tree.total_sections} |",
                    f"| ĞœĞ°ĞºÑ. Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° | {tree.max_depth} |",
                    f"| Actionable | {tree.actionable_sections} |",
                    f"| RACI ÑÑ‚Ğ°Ñ‚ÑƒÑ | {tree.raci_status} |",
                    "",
                    "### Ğ”ĞµÑ€ĞµĞ²Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°",
                    "",
                ])
                
                # Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ´ĞµÑ€ĞµĞ²Ğ°
                def render_tree(node, indent=0):
                    lines = []
                    if node.id != "root":
                        prefix = "  " * indent
                        marker = "ğŸ“Œ" if node.is_actionable else "ğŸ“„"
                        num_part = f"**{node.num}** " if node.num else ""
                        title_short = node.title[:60] + "..." if len(node.title) > 60 else node.title
                        lines.append(f"{prefix}- {marker} {num_part}{title_short}")
                    for child in node.children[:50]:  # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ½Ğ° children
                        lines.extend(render_tree(child, indent + 1))
                    return lines
                
                tree_lines = render_tree(tree.root)
                md_lines.extend(tree_lines[:200])  # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ½Ğ° Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾
                
                if len(tree_lines) > 200:
                    md_lines.append(f"\n*... Ğ¸ ĞµÑ‰Ñ‘ {len(tree_lines) - 200} ÑƒĞ·Ğ»Ğ¾Ğ²*")
            else:
                # Fallback: Ğ¿Ğ»Ğ¾ÑĞºĞ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ Ğ´ĞµÑ€ĞµĞ²Ğ°
                md_lines.extend([
                    "## Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°",
                    "",
                ])
                
                if result.filter_report and result.filter_report.get("kept_important"):
                    md_lines.append("### Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‹")
                    md_lines.append("")
                    for h in result.filter_report["kept_important"][:20]:
                        md_lines.append(f"- {h}")
                    md_lines.append("")
                
                md_lines.append("### Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸")
                md_lines.append("")
                for h in result.headings[:50]:
                    prefix = "#" * min(h.level, 4)
                    md_lines.append(f"{prefix} {h.text}")
                    md_lines.append("")
                
                if len(result.headings) > 50:
                    md_lines.append(f"*... Ğ¸ ĞµÑ‰Ñ‘ {len(result.headings) - 50} Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ²*")
            
            with open(result_dir / "structure.md", "w", encoding="utf-8") as f:
                f.write("\n".join(md_lines))
            
            # 7. ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
            doc_stats = {
                "code": result.doc_code or doc_code,
                "source": result.source,
                "headings": len(result.headings),
            }
            
            if result.source == "docx":
                stats["docx_used"] += 1
            else:
                stats["pdf_used"] += 1
                
                if result.filter_report:
                    fr = result.filter_report
                    stats["filter_stats"]["total_blocks"] += fr.get("total_blocks", 0)
                    stats["filter_stats"]["after_filtering"] += fr.get("after_filtering", 0)
                    stats["filter_stats"]["by_repeat"] += len(fr.get("by_repeat", []))
                    stats["filter_stats"]["by_blacklist"] += len(fr.get("by_blacklist", []))
                    stats["filter_stats"]["by_pattern"] += len(fr.get("by_pattern", []))
                    
                    doc_stats["filtered"] = {
                        "total": fr.get("total_blocks", 0),
                        "kept": fr.get("after_filtering", 0),
                        "by_repeat": len(fr.get("by_repeat", [])),
                        "by_blacklist": len(fr.get("by_blacklist", [])),
                        "by_pattern": len(fr.get("by_pattern", [])),
                    }
            
            stats["documents"].append(doc_stats)
            
            print(f"   âœ… {result.source.upper()}: {len(result.headings)} Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ²")
            
        except Exception as e:
            print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
            stats["documents"].append({
                "code": doc_code,
                "error": str(e),
            })
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹
    print("\nğŸ“Š Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ²...")
    
    # 1. statistics.json
    with open(reports_dir / "statistics.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # 2. summary.txt
    summary_lines = [
        "=" * 70,
        "   Ğ¡Ğ’ĞĞ”ĞĞ«Ğ™ ĞĞ¢Ğ§ĞĞ¢ ĞŸĞ Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ® Ğ“Ğ˜Ğ‘Ğ Ğ˜Ğ”ĞĞĞ“Ğ ĞŸĞĞ Ğ¡Ğ•Ğ Ğ",
        "=" * 70,
        "",
        f"Ğ”Ğ°Ñ‚Ğ°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {base_dir}",
        "",
        "=" * 70,
        "   Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ",
        "=" * 70,
        "",
        f"ğŸ“Š Ğ’ÑĞµĞ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: {stats['total_documents']}",
        f"   ğŸ“„ DOCX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½: {stats['docx_used']}",
        f"   ğŸ“• PDF (fallback): {stats['pdf_used']}",
        "",
        f"ğŸ—‘ï¸ ĞÑ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾:",
        f"   Ğ’ÑĞµĞ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ´Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸: {stats['filter_stats']['total_blocks']}",
        f"   ĞŸĞ¾ÑĞ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸: {stats['filter_stats']['after_filtering']}",
        f"   Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾: {stats['filter_stats']['total_blocks'] - stats['filter_stats']['after_filtering']}",
        "",
        f"   ĞŸĞ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñƒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² (>50%): {stats['filter_stats']['by_repeat']} ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ²",
        f"   ĞŸĞ¾ Ñ‡Ñ‘Ñ€Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿Ğ¸ÑĞºÑƒ: {stats['filter_stats']['by_blacklist']} ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹",
        f"   ĞŸĞ¾ regex-Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼: {stats['filter_stats']['by_pattern']} ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¹",
        "",
        "=" * 70,
        "   Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ«",
        "=" * 70,
        "",
    ]
    
    for doc in stats["documents"]:
        if "error" in doc:
            summary_lines.append(f"âŒ {doc['code']}: ĞĞ¨Ğ˜Ğ‘ĞšĞ - {doc['error']}")
        else:
            source_icon = "ğŸ“„" if doc["source"] == "docx" else "ğŸ“•"
            summary_lines.append(f"{source_icon} {doc['code']}: {doc['headings']} Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ² ({doc['source'].upper()})")
            if "filtered" in doc:
                f = doc["filtered"]
                summary_lines.append(f"   Ğ‘Ğ»Ğ¾ĞºĞ¾Ğ²: {f['total']} â†’ {f['kept']} (Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€: Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ñ‹={f['by_repeat']}, blacklist={f['by_blacklist']}, pattern={f['by_pattern']})")
    
    summary_lines.extend([
        "",
        "=" * 70,
        "   Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ• ĞĞ¢Ğ§ĞĞ¢Ğ«",
        "=" * 70,
        "",
    ])
    
    for report in all_reports:
        summary_lines.append(report)
        summary_lines.append("\n")
    
    with open(reports_dir / "summary.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(summary_lines))
    
    # 3. README.md
    readme = f"""# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°

**Ğ”Ğ°Ñ‚Ğ°:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°

```
output3/hybrid_parser_test/
â”œâ”€â”€ sources/           # Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ PDF Ñ„Ğ°Ğ¹Ğ»Ñ‹
â”œâ”€â”€ results/           # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñƒ
â”‚   â””â”€â”€ NN_DOC-CODE/
â”‚       â”œâ”€â”€ source.pdf          # ĞšĞ¾Ğ¿Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ PDF (Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ)
â”‚       â”œâ”€â”€ source.docx         # ĞšĞ¾Ğ¿Ğ¸Ñ DOCX ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ (Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ)
â”‚       â”œâ”€â”€ parse_result.json   # Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°
â”‚       â”œâ”€â”€ report.txt          # Ğ¢ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
â”‚       â””â”€â”€ structure.md        # Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² Markdown
â””â”€â”€ reports/           # Ğ¡Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹
    â”œâ”€â”€ summary.txt             # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞ²Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
    â””â”€â”€ statistics.json         # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ² JSON
```

## Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°

| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ |
|---------|----------|
| Ğ’ÑĞµĞ³Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² | {stats['total_documents']} |
| DOCX Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½ | {stats['docx_used']} |
| PDF (fallback) | {stats['pdf_used']} |
| Ğ‘Ğ»Ğ¾ĞºĞ¾Ğ² Ğ´Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ | {stats['filter_stats']['total_blocks']} |
| Ğ‘Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ | {stats['filter_stats']['after_filtering']} |

## Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ

| Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ | Ğ¡Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ |
|--------|--------------|
| ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ñ‹ (>50% ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†) | {stats['filter_stats']['by_repeat']} |
| Ğ§Ñ‘Ñ€Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº | {stats['filter_stats']['by_blacklist']} |
| Regex-Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ | {stats['filter_stats']['by_pattern']} |

## Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹

| # | ĞšĞ¾Ğ´ | Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº | Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ² |
|---|-----|----------|------------|
"""
    
    for i, doc in enumerate(stats["documents"], 1):
        if "error" in doc:
            readme += f"| {i} | {doc['code']} | âŒ ĞĞ¨Ğ˜Ğ‘ĞšĞ | - |\n"
        else:
            readme += f"| {i} | {doc['code']} | {doc['source'].upper()} | {doc['headings']} |\n"
    
    with open(output_dir / "README.md", "w", encoding="utf-8") as f:
        f.write(readme)
    
    print(f"\nâœ… Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½: {output_dir}")
    print(f"   ğŸ“ sources/: {len(test_docs)} Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")
    print(f"   ğŸ“ results/: {len(test_docs)} Ğ¿Ğ°Ğ¿Ğ¾Ğº")
    print(f"   ğŸ“ reports/: summary.txt, statistics.json")
    print(f"   ğŸ“„ README.md")


if __name__ == "__main__":
    base_dir = Path("/home/budnik_an/Obligations/input2/BND")
    output_dir = Path("/home/budnik_an/Obligations/output3/hybrid_parser_test")
    
    count = int(sys.argv[1]) if len(sys.argv) > 1 else 8
    
    export_test_results(base_dir, output_dir, count)
