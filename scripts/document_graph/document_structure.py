"""
–¢–∏–ø–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –°–ú–ö (–°–∏—Å—Ç–µ–º–∞ –ú–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ –ö–∞—á–µ—Å—Ç–≤–∞)

–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
- –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –ú—É—Å–æ—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã)
- –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫—É
"""

from dataclasses import dataclass, field
from typing import List, Set, Dict, Optional
from enum import Enum
import re


class DocumentType(Enum):
    """–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –°–ú–ö"""
    KD = "–ö–î"      # –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    DP = "–î–ü"      # –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
    IOT = "–ò–û–¢"    # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –æ—Ö—Ä–∞–Ω–µ —Ç—Ä—É–¥–∞
    RD = "–†–î"      # –†—É–∫–æ–≤–æ–¥—è—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    RI = "–†–ò"      # –†–∞–±–æ—á–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    ST = "–°–¢"      # –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã
    RG = "–†–ì"      # –†–µ–≥–ª–∞–º–µ–Ω—Ç—ã
    PR = "–ü–†"      # –ü—Ä–∞–≤–∏–ª–∞
    TPM = "TPM"    # TPM –¥–æ–∫—É–º–µ–Ω—Ç—ã
    UNKNOWN = "UNKNOWN"


@dataclass
class StandardSection:
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    number: int
    title_ru: str
    title_en: Optional[str] = None
    is_mandatory: bool = True
    description: str = ""


# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –°–ú–ö
STANDARD_SECTIONS = [
    StandardSection(1, "–¶–ï–õ–¨ –ò –û–ë–õ–ê–°–¢–¨ –ü–†–ò–ú–ï–ù–ï–ù–ò–Ø", "PURPOSE AND SCOPE", True, 
                    "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏ –æ–±–ª–∞—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"),
    StandardSection(2, "–ù–û–†–ú–ê–¢–ò–í–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´", "NORMATIVE REFERENCES", True,
                    "–°—Å—ã–ª–∫–∏ –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"),
    StandardSection(3, "–û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø", "DEFINITIONS", True,
                    "–¢–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"),
    StandardSection(4, "–û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø –ò –°–û–ö–†–ê–©–ï–ù–ò–Ø", "ABBREVIATIONS", True,
                    "–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è"),
    StandardSection(5, "–û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø", "GENERAL PROVISIONS", True,
                    "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ—Ü–µ—Å—Å–µ"),
    StandardSection(6, "–û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨", "RESPONSIBILITY", True,
                    "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏"),
]

# –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –¥–ª—è –ò–û–¢
IOT_SECTIONS = [
    StandardSection(1, "–û–ë–©–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê", None, True),
    StandardSection(2, "–¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê –ü–ï–†–ï–î –ù–ê–ß–ê–õ–û–ú –†–ê–ë–û–¢–´", None, True),
    StandardSection(3, "–¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê –í–û –í–†–ï–ú–Ø –†–ê–ë–û–¢–´", None, True),
    StandardSection(4, "–¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê –í –ê–í–ê–†–ò–ô–ù–´–• –°–ò–¢–£–ê–¶–ò–Ø–•", None, True),
    StandardSection(5, "–¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê –ü–û –û–ö–û–ù–ß–ê–ù–ò–ò –†–ê–ë–û–¢–´", None, True),
    StandardSection(6, "–ü–ï–†–ï–ß–ï–ù–¨ –ù–û–†–ú–ê–¢–ò–í–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í", None, False),
]


# =============================================================================
# –ú–£–°–û–†–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ (–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã)
# =============================================================================

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, case-insensitive)
GARBAGE_EXACT_PATTERNS = {
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã –ö–î
    "PROCEDURES",
    "–°OLLECTION OF CONTINUING AIRWORTHINESS",
    "COLLECTION OF CONTINUING AIRWORTHINESS",
    
    # –†—É—Å—Å–∫–∏–µ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã
    "–°–ë–û–†–ù–ò–ö –ü–†–û–¶–ï–î–£–† –ü–û –ü–û–î–î–ï–†–ñ–ê–ù–ò–Æ –õ–ï–¢–ù–û–ô",
    "–ì–û–î–ù–û–°–¢–ò",
    
    # –®–∞–ø–∫–∏ —Ç–∞–±–ª–∏—Ü
    "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
    "‚Ññ —Ä. —Ç.",
    "‚Ññ —ç–∫–∑./ ‚Ññ –∫.—Ç.",
    "‚Ññ  —ç–∫–∑./ ‚Ññ  –∫.—Ç.",
    "‚Ññ –∫.—Ç.",
    "‚Ññ —Ä.—Ç.",
    
    # –ö–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
    "–°—Ç—Ä.",
    "Page",
    "—Å—Ç—Ä.",
    "page",
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (regex)
GARBAGE_REGEX_PATTERNS = [
    # –ù–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–∞—Ö (–ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ)
    r"^(–ö–î|–î–ü|–†–î|–†–ò|–°–¢|–†–ì|–ò–û–¢)-[–ê-–ØA-Z0-9\.\-]+\s*(KD|DP|RD|RI|ST|RG)?-?[A-Z0-9\.\-]*$",
    
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π
    r"^[A-Z][a-z]+.*Manual$",
    r"^[A-Z][a-z]+.*Procedures?$",
    r"^[A-Z][a-z]+.*Program$",
    
    # –ù–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
    r"^Page/–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+\d+",
    r"^–°—Ç—Ä\.\s*/\s*page\s+\d+",
    r"^\d+-\d+-\d+$",  # –§–æ—Ä–º–∞—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–∏–ø–∞ "12-4-22"
    
    # –î–∞—Ç—ã –≤–≤–µ–¥–µ–Ω–∏—è
    r"^Effect\./–î–∞—Ç–∞ –≤–≤–µ–¥\.\s+\d+\.\d+\.\d+",
    r"^–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è",
    
    # –†–µ–≤–∏–∑–∏–∏
    r"^Rev\./–†–µ–≤–∏–∑–∏—è\s+\d+",
    r"^Issue/–ò–∑–¥–∞–Ω–∏–µ\s+\d+",
    
    # –†–∞–∑–±–∏—Ç—ã–µ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å–ª–æ–≤)
    r"^—Å—É–¥–æ–≤$",
    r"^–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—é$",
    r"^—Ç–µ—Ö–Ω–∏–∫–∏$",
    r"^—ç–∫–∏–ø–∞–∂–∞–º–∏$",
    r"^–≤–æ–∑–¥—É—à–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–∑–æ–∫$",
    r"^—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–º—É—â–µ—Å—Ç–≤–∞$",
    r"^—Å–∫–ª–∞–¥—Å–∫–æ–≥–æ —É—á–µ—Ç–∞ –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω–æ-$",
    r"^\(—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤\)$",
    
    # –ü—É—Å—Ç—ã–µ —è–∫–æ—Ä—è markdown
    r"^\{#[a-z0-9\-]+\}$",
    r"^`\s*$",  # –ü—É—Å—Ç—ã–µ –±–ª–æ–∫–∏ –∫–æ–¥–∞
    
    # –®–∞–ø–∫–∏ —Ç–∞–±–ª–∏—Ü –∏ —Ñ–æ—Ä–º
    r"^–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è\s*/\s*Implemented",
    r"^Check list of",
    r"^FORM\s+UT\d+",
    r"^Form\s+UT\d+",
    r"^‚Ññ\s*[‚Äì-]\s*[¬¨_]+",  # –ü—É—Å—Ç—ã–µ –ø–æ–ª—è —Ñ–æ—Ä–º (‚Ññ ‚Äì ¬¨___.__/__)
    r"^–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ñ–æ—Ä–º—ã",
    r"^–í–æ–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è",
    r"^–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ",
    r"^–°—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
    r"^–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞",
    r"^–°—Å—ã–ª–∫–∞ –Ω–∞ –†–î",
    r"^–°—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç",
    
    # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–æ–≤
    r"^–õ–ï–¢–ù–û–ô –ì–û–î–ù–û–°–¢–ò$",
    r"^CONTINUING AIRWORTHINESS$",
    r"^AIRWORTHINESS$",
    
    # –°–æ–∫—Ä–∞—â–µ–Ω–∏—è –ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–π
    r"^–¶–£–¢–û$",
    r"^–£–ü–õ–ì$",
    r"^–ù–ï–¢$",
    r"^N/A$",
    
    # –†–∏—Å—É–Ω–∫–∏ –±–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è
    r"^–†–∏—Å\.\s*\d+$",
    r"^Fig\.\s*\d+$",
    
    # –®–∞–ø–∫–∏ —Ç–∞–±–ª–∏—Ü –∞–≤–∏–∞—Ü–∏–æ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
    r"^REGISTRATION",
    r"^PART NUMBER$",
    r"^–°–ï–†–ò–ô–ù–´–ô –ù–û–ú–ï–†",
    r"^–¢–ò–ü –í–°",
    r"^AIRCRAFT TYPE",
    r"^–ß–ï–†–¢–ï–ñ–ù–´–ô –ù–û–ú–ï–†",
    r"^–¶–ò–ö–õ–´$",
    
    # until Status % of completed... (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–∞–±–ª–∏—Ü)
    r"^until\s+Status",
]


# =============================================================================
# –í–ê–ñ–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ (—Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å)
# =============================================================================

IMPORTANT_PATTERNS = [
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –°–ú–ö
    r"^\d+\s+(–¶–ï–õ–¨|–û–ë–õ–ê–°–¢–¨|–ù–û–†–ú–ê–¢–ò–í–ù–´–ï|–û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø|–û–ë–û–ó–ù–ê–ß–ï–ù–ò–Ø|–û–ë–©–ò–ï|–û–¢–í–ï–¢–°–¢–í–ï–ù–ù–û–°–¢–¨|–ó–ê–ü–ò–°–ò)",
    
    # –†–∞–∑–¥–µ–ª—ã –ò–û–¢
    r"^\d+\s+–¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê",
    r"^\d+\s+–û–ë–©–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø",
    r"^\d+\s+–ü–ï–†–ï–ß–ï–ù–¨ –ù–û–†–ú–ê–¢–ò–í–ù–´–•",
    
    # –°–ª—É–∂–µ–±–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
    r"^–ü–†–ï–î–ò–°–õ–û–í–ò–ï",
    r"^–ü–ï–†–ï–ß–ï–ù–¨ –†–ê–°–°–´–õ–ö–ò",
    r"^–°–û–î–ï–†–ñ–ê–ù–ò–ï$",
    r"^–û–ì–õ–ê–í–õ–ï–ù–ò–ï$",
    r"^–í–í–ï–î–ï–ù–ò–ï$",
    
    # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    r"^–ü–†–ò–õ–û–ñ–ï–ù–ò–ï\s+[–ê-–ØA-Z0-9]",
    r"^–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\s+[–ê-–ØA-Z0-9]",
]


class DocumentStructureAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
    - –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∫–æ–¥—É
    - –ú—É—Å–æ—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    - –í–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
    """
    
    def __init__(self):
        self._garbage_exact = {p.upper() for p in GARBAGE_EXACT_PATTERNS}
        self._garbage_regex = [re.compile(p, re.IGNORECASE) for p in GARBAGE_REGEX_PATTERNS]
        self._important_regex = [re.compile(p, re.IGNORECASE) for p in IMPORTANT_PATTERNS]
    
    def get_document_type(self, doc_code: str) -> DocumentType:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∫–æ–¥—É"""
        if not doc_code:
            return DocumentType.UNKNOWN
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å
        parts = doc_code.split("-")
        if not parts:
            return DocumentType.UNKNOWN
            
        prefix = parts[0].upper()
        
        type_map = {
            "–ö–î": DocumentType.KD,
            "–î–ü": DocumentType.DP,
            "–ò–û–¢": DocumentType.IOT,
            "–†–î": DocumentType.RD,
            "–†–ò": DocumentType.RI,
            "–°–¢": DocumentType.ST,
            "–†–ì": DocumentType.RG,
            "–ü–†": DocumentType.PR,
            "TPM": DocumentType.TPM,
        }
        
        return type_map.get(prefix, DocumentType.UNKNOWN)
    
    def is_garbage(self, text: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –º—É—Å–æ—Ä–æ–º (–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —ç–ª–µ–º–µ–Ω—Ç)
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ)
            
        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ –º—É—Å–æ—Ä
        """
        if not text:
            return True
            
        # –û—á–∏—â–∞–µ–º –æ—Ç markdown —è–∫–æ—Ä–µ–π
        clean_text = re.sub(r'\s*\{#[^}]+\}\s*$', '', text).strip()
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç (–æ–±—ã—á–Ω–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç)
        if len(clean_text) < 3:
            return True
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if clean_text.upper() in self._garbage_exact:
            return True
        
        # Regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in self._garbage_regex:
            if pattern.search(clean_text):
                return True
        
        return False
    
    def is_important(self, text: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≤–∞–∂–Ω—ã–º —Ä–∞–∑–¥–µ–ª–æ–º
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ –≤–∞–∂–Ω—ã–π —Ä–∞–∑–¥–µ–ª
        """
        if not text:
            return False
            
        clean_text = re.sub(r'\s*\{#[^}]+\}\s*$', '', text).strip()
        
        for pattern in self._important_regex:
            if pattern.search(clean_text):
                return True
        
        return False
    
    def classify_heading(self, text: str) -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫
        
        Returns:
            "garbage" - –º—É—Å–æ—Ä (–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª)
            "important" - –≤–∞–∂–Ω—ã–π —Ä–∞–∑–¥–µ–ª
            "content" - –∫–æ–Ω—Ç–µ–Ω—Ç (–Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å)
        """
        if self.is_garbage(text):
            return "garbage"
        if self.is_important(text):
            return "important"
        return "content"
    
    def get_standard_sections(self, doc_type: DocumentType) -> List[StandardSection]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if doc_type == DocumentType.IOT:
            return IOT_SECTIONS
        return STANDARD_SECTIONS


# –°–æ–∑–¥–∞—ë–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
analyzer = DocumentStructureAnalyzer()


def filter_garbage_headings(headings: List[str]) -> List[str]:
    """
    –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –º—É—Å–æ—Ä–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    
    Args:
        headings: –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        
    Returns:
        –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    """
    return [h for h in headings if not analyzer.is_garbage(h)]


def detect_headers_footers(pages_text: List[List[str]], threshold_percent: float = 50.0) -> Set[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç—ã, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –Ω–∞ >50% —Å—Ç—Ä–∞–Ω–∏—Ü (–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã)
    
    Args:
        pages_text: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–∞–∂–¥–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤
        threshold_percent: –ü–æ—Ä–æ–≥ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50%)
        
    Returns:
        –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤-–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–æ–≤
    """
    if not pages_text:
        return set()
    
    total_pages = len(pages_text)
    threshold = total_pages * (threshold_percent / 100.0)
    
    # –°—á–∏—Ç–∞–µ–º –Ω–∞ —Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π –±–ª–æ–∫
    from collections import defaultdict
    text_page_count = defaultdict(int)
    
    for page_blocks in pages_text:
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (—á—Ç–æ–±—ã –Ω–µ —Å—á–∏—Ç–∞—Ç—å –¥–≤–∞–∂–¥—ã)
        seen_on_page = set()
        for block in page_blocks:
            normalized = normalize_text(block)
            if normalized and normalized not in seen_on_page:
                text_page_count[normalized] += 1
                seen_on_page.add(normalized)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã
    garbage = {text for text, count in text_page_count.items() 
               if count > threshold}
    
    return garbage


def normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    
    - –£–±–∏—Ä–∞–µ—Ç markdown —è–∫–æ—Ä—è
    - –ü—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    - –£–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    """
    if not text:
        return ""
    
    # –£–±–∏—Ä–∞–µ–º markdown —è–∫–æ—Ä—è
    clean = re.sub(r'\s*\{#[^}]+\}\s*$', '', text)
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    clean = ' '.join(clean.split())
    # –ù–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    clean = clean.lower().strip()
    
    return clean


def filter_with_report(headings: List[str], repeat_garbage: Set[str] = None) -> Dict:
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –æ—Ç—á—ë—Ç–æ–º –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∏–ª—å—Ç—Ä—É
    
    Args:
        headings: –°–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        repeat_garbage: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª–æ–≤ (–∏–∑ detect_headers_footers)
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:
        - filtered_by_repeat: –≤—ã–∫–∏–Ω—É—Ç–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –ø–æ–≤—Ç–æ—Ä–æ–≤
        - filtered_by_blacklist: –≤—ã–∫–∏–Ω—É—Ç–æ –ø–æ —á—ë—Ä–Ω–æ–º—É —Å–ø–∏—Å–∫—É
        - filtered_by_pattern: –≤—ã–∫–∏–Ω—É—Ç–æ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        - kept_important: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ (–≤–∞–∂–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã)
        - kept_content: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ (–∫–æ–Ω—Ç–µ–Ω—Ç)
    """
    from collections import Counter
    
    repeat_garbage = repeat_garbage or set()
    
    result = {
        "filtered_by_repeat": [],
        "filtered_by_blacklist": [],
        "filtered_by_pattern": [],
        "kept_important": [],
        "kept_content": [],
    }
    
    for heading in headings:
        normalized = normalize_text(heading)
        clean_text = re.sub(r'\s*\{#[^}]+\}\s*$', '', heading).strip()
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—ã (>50% —Å—Ç—Ä–∞–Ω–∏—Ü)
        if normalized in repeat_garbage:
            result["filtered_by_repeat"].append(heading)
            continue
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —á—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ (—Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
        if clean_text.upper() in analyzer._garbage_exact:
            result["filtered_by_blacklist"].append(heading)
            continue
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º—É—Å–æ—Ä–∞ (regex)
        is_pattern_garbage = False
        for pattern in analyzer._garbage_regex:
            if pattern.search(clean_text):
                result["filtered_by_pattern"].append(heading)
                is_pattern_garbage = True
                break
        
        if is_pattern_garbage:
            continue
        
        # 4. –ï—Å–ª–∏ –ø—Ä–æ—à—ë–ª –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã - –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
        if analyzer.is_important(heading):
            result["kept_important"].append(heading)
        else:
            result["kept_content"].append(heading)
    
    return result


def analyze_document_structure(md_content: str) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    Args:
        md_content: Markdown –∫–æ–Ω—Ç–µ–Ω—Ç
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
    """
    lines = md_content.split('\n')
    
    stats = {
        "total_headings": 0,
        "garbage_headings": 0,
        "important_headings": 0,
        "content_headings": 0,
        "garbage_examples": [],
        "important_examples": [],
    }
    
    for line in lines:
        if line.startswith('# '):
            heading = line[2:].strip()
            stats["total_headings"] += 1
            
            classification = analyzer.classify_heading(heading)
            
            if classification == "garbage":
                stats["garbage_headings"] += 1
                if len(stats["garbage_examples"]) < 5:
                    stats["garbage_examples"].append(heading)
                    
            elif classification == "important":
                stats["important_headings"] += 1
                if len(stats["important_examples"]) < 5:
                    stats["important_examples"].append(heading)
                    
            else:
                stats["content_headings"] += 1
    
    return stats


if __name__ == "__main__":
    # –¢–µ—Å—Ç –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    test_headings = [
        "# PROCEDURES",
        "# –°OLLECTION OF CONTINUING AIRWORTHINESS",
        "# 1 –¶–ï–õ–¨ –ò –û–ë–õ–ê–°–¢–¨ –ü–†–ò–ú–ï–ù–ï–ù–ò–Ø",
        "# 3 –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø",
        "# –ö–î-–†–î-–ë1.043-02  KD-RD-B1.043-02",
        "# —Å—É–¥–æ–≤",
        "# –ü–†–ï–î–ò–°–õ–û–í–ò–ï",
        "# 4 –¢–†–ï–ë–û–í–ê–ù–ò–Ø –û–•–†–ê–ù–´ –¢–†–£–î–ê –í –ê–í–ê–†–ò–ô–ù–´–• –°–ò–¢–£–ê–¶–ò–Ø–•",
        "# –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
    ]
    
    print("=== –¢–ï–°–¢ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò ===\n")
    
    for h in test_headings:
        text = h[2:]  # –£–±–∏—Ä–∞–µ–º "# "
        classification = analyzer.classify_heading(text)
        emoji = {"garbage": "üóëÔ∏è", "important": "‚úÖ", "content": "üìÑ"}[classification]
        print(f"{emoji} [{classification:10}] {text[:50]}")
